{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimal Portfolios: Sensitivity to Inputs  {#sec-input-sensitivity}\n",
        "\n",
        "Optimal risky portfolios can be quite sensitive to the inputs, which consist of expected returns, standard deviations, and correlations across assets.  An example is provided in the first section below.  In practice, one must estimate these inputs.  Expected returns are notoriously hard to estimate using historical data, which we discuss in detail in SECTION LINK, so we often resort to models for inputs here or we assume expected returns are equal across assets.  Asset volatilities and correlations can be forecasted more sucessfully.  Forecasting correlations in portfolios with many assets is complicated by the fact that the number of correlations we need to estimate rises quadratically with the number of assets.\n",
        "\n",
        "## Sensitivity of weights\n",
        "\n",
        "Let's return to the example of investing in US equity, developed international, and emerging market funds from @sec-optimal-portfolios-theory.  We will fix the input parameters for all but the US equity portfolio and see what happens to the tangency portfolio weights as we vary our assumptions about the US equity portfolio.\n",
        "\n",
        "### Expected returns {-}\n",
        "\n",
        "Portfolio optimization is quite sensitive to expected returns.  This is rather unfortunate because financial economists and practitioners are not particularly great at forecasting expected returns.  @fig-sensitivity-expret shows how the tangency portfolio weight changes as a function of different inputs for the US equity expected return.  The tangency portfolio invests nothing in US equity with a US equity expected return input of about 5.5%, but allocates 100% to US equity with an input of about 7.5%.  If the expected return input is 5%, the optimal risky portfolio has short exposure equal to about 50% of total capital.  Thus, a 2.5% difference in expected return input leads to a possible difference in portfolio weight for US equities of 150%!  A 2.5% difference in estimated expected return is not extremely large relative to the precision with which expected returns are estimated.  \n"
      ],
      "id": "71144ac2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-expret\n",
        "#| fig-cap: Sensitivity of tangency weights to expected return input\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "##### Inputs\n",
        "# Risk-free rate\n",
        "r = 0.02\n",
        "# Expected returns\n",
        "means = np.array([0.06, 0.065, 0.08])\n",
        "# Standard deviations\n",
        "sds = np.array([0.15, 0.165, 0.21])\n",
        "# Correlations\n",
        "corr12 = 0.75\n",
        "corr13 = 0.75\n",
        "corr23 = 0.75\n",
        "# Covariance matrix\n",
        "C  = np.identity(3)\n",
        "C[0, 1] = C[1, 0] = corr12\n",
        "C[0, 2] = C[2, 0] = corr13\n",
        "C[1, 2] = C[2, 1] = corr23\n",
        "cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "def tangency(means, cov, rf, Shorts):\n",
        "    n = len(means)\n",
        "    def f(w):\n",
        "        mn = w @ means\n",
        "        sd = np.sqrt(w.T @ cov @ w)\n",
        "        return -(mn - rf) / sd\n",
        "    # Initial guess (equal-weighted)\n",
        "    w0 = (1/n)*np.ones(n)\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = np.ones(n)\n",
        "    b = 1\n",
        "    cons = [{\"type\": \"eq\", \"fun\": lambda x: A @ x - b}]\n",
        "    if Shorts==True:\n",
        "        # No short-sale constraint\n",
        "        bnds = [(None, None) for i in range(n)] \n",
        "    else:\n",
        "        # With short-sale constraint\n",
        "        bnds = [(0, None) for i in range(n)] \n",
        "    # Optimization\n",
        "    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x\n",
        "    return wgts_tangency\n",
        "\n",
        "wgts_true = tangency(means,cov,r,Shorts=True)\n",
        "\n",
        "# Tangency portfolios for a range of assumed asset 1 expected returns\n",
        "n = len(means)\n",
        "num_grid=100\n",
        "asset1_means = np.linspace(0.04,0.10,num_grid)\n",
        "wgts = np.zeros((num_grid,n))\n",
        "\n",
        "for i,m in enumerate(asset1_means):\n",
        "    wgts[i] = tangency(np.array([m, means[1], means[2]]),cov,r,Shorts=True)\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Asset 1 Expected Return Input = %{x:0.2%}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=asset1_means,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assume input\n",
        "trace_true = go.Scatter(x=[means[0]],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "# fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Asset 1 Expected Return Input\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-expret",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Deviations {-}\n",
        "\n",
        "@fig-sensitivity-sd shows the sensitivity of the US market allocation in the tangency portfolio for different standard deviation inputs.  For values in the region of the historical average volatility of 15%, higher standard deviation inputs result in lower allocations to US equities.\n"
      ],
      "id": "83ab8783"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-sd\n",
        "#| fig-cap: Sensitivity of tangency weights to standard deviation input\n",
        "##### Variance input of asset 1:\n",
        "# Tangency portfolios for a range of assumed asset 1 standard deviations\n",
        "asset1_sds = np.linspace(0.05,0.25,num_grid)\n",
        "wgts = np.zeros((num_grid,n))\n",
        "\n",
        "for i,s in enumerate(asset1_sds):\n",
        "    sds_new = np.array([s, sds[1], sds[2]])\n",
        "    wgts[i] = tangency(means,np.diag(sds_new) @ C @ np.diag(sds_new),r,Shorts=True)\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Asset 1 Standard Deviation Input = %{x}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=asset1_sds,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assumed input\n",
        "trace_true = go.Scatter(x=[sds[0]],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "# fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Asset 1 Standard Deviation Input\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.55))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-sd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlations {-}\n",
        "\n",
        "@fig-sensitivity-corr shows the sensitivity of the US market's weight in the tangency portfolio for different inputs of the correlation between the US equity fund and the developed international equity fund.  Lower correlations imply greater diversification benefits, so more capital is allocated to US equities for lower correlation levels, all else equal.\n"
      ],
      "id": "2a282272"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-corr\n",
        "#| fig-cap: Sensitivity of tangency weights to standard deviation input\n",
        "##### Correlation of assets 1 and 2:\n",
        "# Tangency portfolios for a range of assumed asset 1 standard deviations\n",
        "corr12_grid = np.linspace(0.15,0.95,num_grid)\n",
        "wgts = np.empty((num_grid,n))\n",
        "\n",
        "def is_pos_def(x):\n",
        "    if np.all(np.linalg.eigvals(x) > 0):\n",
        "        return 'True'\n",
        "    else:\n",
        "        return 'False'\n",
        "\n",
        "for i,c in enumerate(corr12_grid):\n",
        "    # Covariance matrix\n",
        "    C  = np.identity(3)\n",
        "    C[0, 1] = C[1, 0] = c\n",
        "    C[0, 2] = C[2, 0] = corr13\n",
        "    C[1, 2] = C[2, 1] = corr23\n",
        "    # Check feasible correlations\n",
        "    if is_pos_def(C):\n",
        "        wgts[i] = tangency(means,np.diag(sds) @ C @ np.diag(sds),r,Shorts=True)\n",
        "    else:\n",
        "        print(\"not positive definite\" + str(c*100))\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Input: Correlation of Assets 1 and 2 = %{x}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=corr12_grid,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assumed input\n",
        "trace_true = go.Scatter(x=[corr12],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "# fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Correlation of Assets 1 and 2\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.55))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-corr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Maximization {-}\n",
        "\n",
        "The sensitivity of the tangency portfolio to each of these inputs presents a practical problem for investors seeking to implement optimal portfolios in a quantitative sense.  Portfolio optimization is sometimes referred to as error maximization because the tangency portfolio weights may reflect errors in the inputs rather than optimal diversification and risk-reward trade-offs.\n",
        "\n",
        "When the estimated expected return for an asset is too high relative to its true (but unknown) value, the resulting tangency portfolio will put too much weight on that asset, relative to the unknown *true* tangency portfolio.  Similarly, the tangency portfolio weight will be too low for an asset with an over-estimated variance relative to the tangency portfolio calculated using the true (but unknown) variance.  Finally, optimal portfolios will overallocate capital to assets with underestimated correlations and underallocate capital to assets with overestimated correlations.  The error maximization problem becomes more acute for portfolios with many assets simply because the increase in the number of correlations to be estimated increases the likelihood of substantial errors.\n",
        "\n",
        "## Estimating the Input List\n",
        "\n",
        "A simple way to estimate expected return, standard deviations, and correlations is to use historical data.  Specifically, the historical arithmetic average return is an estimate of an asset's expected return.  The historical standard deviation is an estimate of the volatility of the asset's return.  Historical correlations across assets can be estimated using the historical return series of each pair of assets.\n",
        "\n",
        "To understand the estimation issues that arise in practice, we can simulate the performance of various strategies for using historical data to estimate the input parameters.  We will simulate from an assumed distribution and then use this simulated \"historical\" data to estimate inputs to our portfolio optimization problem each year.  We assume an investment horizon of 50 years.  We will consider the following strategies for estimating inputs:\n",
        "\n",
        "* **Est-All**: use historical data to estimate expected returns, standard deviations, and correlations.  The optimal risky portfolio is the tangency portfolio, which is scaled up or down depending on risk aversion.\n",
        "\n",
        "* **Est-SD-Corr**: use historical data to estimate standard deviations and correlations, and assume expected returns are the same across all assets.  Under this assumption, the optimal risky portfolio is the global minimum variance portfolio.  For the purposes of determining optimal capital allocation based on risk aversion, we use the cross-sectional average of the historical time-series average return.  \n",
        "\n",
        "* **Est-SD**: use historical data to estimate standard deviations only, and assume correlations across assets are zero and expected returns are the same across all assets and correlations.  This strategy is known as risk parity. As in Est-SD-Corr, we use the cross-sectional average of the historical time-series average return as the expected return input for the purposes of determining optimal capital allocation based on risk aversion. \n",
        "\n",
        "* **Est-None**: do not use historical data to estimate expected returns, standard deviations, or correlations. If we assume that all assets have the same expected returns and standard deviations (with zero correlation across assets), the optimal portfolio is an equal-weighted portfolio of the assets. This is also referred to as the $1/N$ portfolio.   For the purposes of determining optimal capital allocation based on risk aversion, we use the cross-sectional averages of the historical time-series return mean and standard deviation as the expected return and standard deviation inputs. \n",
        "\n",
        "The four estimation strategies represent differing levels of ambition in terms of using historical data to estimate inputs to the portfolio optimization problem.  Below, we provide some examples to demonstrate potential benefits and costs of attempting to estimate all or part of the input list using historical estimates.\n",
        "\n",
        "\n",
        "### Length of the Estimation Window {-}\n",
        "\n",
        "First, let's consider the effect of the length of the estimation window on implementing each of the strategies.  Each year, we'll use the last $T$ years of data to estimate the inputs for a three-asset portfolio, rebalancing our portfolio to the optimal portfolio based on the new input estimates found each year.  We consider estimation window lengths of $T$=10, 20, 30, 40, and 50 years.  We consider two scenarios for the amount of dispersion in true expected returns; the simulations summarized in @fig-input-est-window have more expected return disperion than those described in @fig-input-est-window2.\n",
        "\n",
        "@fig-input-est-window shows the realized Sharpe ratios of the strategies as a function of the window length.  The displayed values are the averages from 100 simulations.  As a benchmark, we also show the average realized Sharpe ratio of using the true optimal tangency portfolio weights.  Note that these weights assume knowledge of the data-generating process not available to investors in practice.\n",
        "\n",
        "The figure shows that using a longer time-series to estimate expected returns, standard deviations, and correlations results in better realized performance regardless of the estimation strategy used.^[Recall that Est-None does not use historical estimates in forming portfolio weights within the risky portfolio; it weights risky assets equally. Est-None does rely on cross-sectional averages of historical estimates to allocate capital between the risky portfolio and the risk-free asset.  The improvement with window length for Est-None is due to better capital allocation.]  The improvement is particularly pronounced for Est-All.  This is not surprising as this is the only strategy that uses historical averages as part of the input list.  Sample averages are fairly noisy estimates of population means in small samples, so longer estimation windows are valuable.^[It is worth noting that we are assuming returns are iid (independently and identically distributed) in these simulations.  As such, more data results in better estimates of expected returns (i.e., the population mean).  If expected returns are time-varying, this need not be the case.]\n"
      ],
      "id": "aa5e8c92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-window\n",
        "#| fig-cap: Simulated performance of various strategies as a function of input estimation window length--more dispersion in expected returns\n",
        "\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import pandas as pd\n",
        "\n",
        "def tangency(means, cov, rf, short_lb):\n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''\n",
        "\n",
        "    n = len(means)\n",
        "    def f(w):\n",
        "        mn = w @ means\n",
        "        sd = np.sqrt(w.T @ cov @ w)\n",
        "        return -(mn - rf) / sd\n",
        "    # Initial guess (equal-weighted)\n",
        "    w0 = (1/n)*np.ones(n)\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = np.ones(n)\n",
        "    b = 1\n",
        "    cons = [{\"type\": \"eq\", \"fun\": lambda x: A @ x - b}]\n",
        "    bnds = [(short_lb, None) for i in range(n)] \n",
        "    # Optimization\n",
        "    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x\n",
        "    return wgts_tangency\n",
        "\n",
        "def gmv(cov, short_lb): \n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''    \n",
        "    n = len(cov)\n",
        "    Q = matrix(cov, tc=\"d\")\n",
        "    p = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    if short_lb==None:\n",
        "        # No position limits\n",
        "        G = matrix(np.zeros((n,n)), tc=\"d\")\n",
        "        h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    else:\n",
        "        # Constraint: short-sales not allowed\n",
        "        G = matrix(-np.identity(n), tc=\"d\")\n",
        "        h = matrix(-short_lb * np.ones(n), (n, 1), tc=\"d\")\n",
        "    # Fully-invested constraint\n",
        "    A = matrix(np.ones(n), (1, n), tc=\"d\")\n",
        "    b = matrix([1], (1, 1), tc=\"d\")\n",
        "    sol = Solver(Q, p, G, h, A, b, options={'show_progress': False})\n",
        "    wgts_gmv = np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else np.array(n * [np.nan])\n",
        "    return wgts_gmv\n",
        "\n",
        "# Simulation function\n",
        "def simulation(means, cov, short_lb, seed, window):\n",
        "\trets = mvn.rvs(means, cov, size=window+T, random_state = seed)\n",
        "\tdf = pd.DataFrame(data=rets, columns=['r0','r1','r2'])\n",
        "\tdf.columns\n",
        "\tdf['mn0']=df['r0'].rolling(window).mean()\n",
        "\tdf['mn1']=df['r1'].rolling(window).mean()\n",
        "\tdf['mn2']=df['r2'].rolling(window).mean()\n",
        "\tdf['sd0']=df['r0'].rolling(window).std()\n",
        "\tdf['sd1']=df['r1'].rolling(window).std()\n",
        "\tdf['sd2']=df['r2'].rolling(window).std()\n",
        "\n",
        "\tcorrs = df[['r0','r1','r2']].rolling(window, min_periods=window).corr()\n",
        "\tdf['c01']=corrs.loc[(slice(None),'r0'),'r1'].values\n",
        "\tdf['c02']=corrs.loc[(slice(None),'r0'),'r2'].values\n",
        "\tdf['c12']=corrs.loc[(slice(None),'r1'),'r2'].values\n",
        "    \n",
        "\twgts_true = tangency(means,cov,r,short_lb)\n",
        "\twgt_cal_true = (wgts_true @ means - r) / (raver * (wgts_true @ cov @ wgts_true))\n",
        "\n",
        "\n",
        "\tmodel_list = ['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd']\n",
        "\tfor model in model_list:\n",
        "\t\tdf['portret_'+model] = np.nan  # portret is the realized portfolio return of the 100% risky asset portfolio\n",
        "\t\tif model not in ['true','est_none']:\n",
        "\t\t\tdf['wgt0_'+model] = np.nan\n",
        "\t\t\tdf['wgt1_'+model] = np.nan\n",
        "\t\t\tdf['wgt2_'+model] = np.nan\n",
        "\t\tdf['wgt_cal_'+model] =np.nan\n",
        "\t\tdf['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor\n",
        "\n",
        "\tfor i in np.arange(window,window+T):\n",
        "\t\t# Full estimation inputs at each point in time\n",
        "\t\tmeans = df[['mn0','mn1','mn2']].iloc[i-1].values\n",
        "\t\tsds   = df[['sd0','sd1','sd2']].iloc[i-1].values\n",
        "\t\tcorr01 = df.loc[i-1,'c01']\n",
        "\t\tcorr02 = df.loc[i-1,'c02']\n",
        "\t\tcorr12 = df.loc[i-1,'c12']\n",
        "\t\tC  = np.identity(3)\n",
        "\t\tC[0, 1] = C[1, 0] = corr01\n",
        "\t\tC[0, 2] = C[2, 0] = corr02\n",
        "\t\tC[1, 2] = C[2, 1] = corr12\n",
        "\t\tcov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "\t\t##### Note: all portfolio weights considered to be beginning of period weights\n",
        "\t\t##### (so multiply by contemporaneous realized returns)\n",
        "\t\t# Theoretical optimal weights\n",
        "\t\tmodel = 'true'\n",
        "\t\tdf.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_true\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + wgt_cal_true * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Full estimation tangency portfolio\n",
        "\t\tmodel = 'est_all'\n",
        "\t\tw0, w1, w2 = tangency(means,cov,r,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Estimate only covariance matrix\n",
        "\t\tmodel = 'est_sd_corr'\n",
        "\t\tw0, w1, w2 = gmv(cov,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\n",
        "\t\t# Estimate only standard deviations in covariance matrix\n",
        "\t\tmodel = 'est_sd'\n",
        "\t\tcov[0, 1] = cov[1, 0] = 0.0\n",
        "\t\tcov[0, 2] = cov[2, 0] = 0.0\n",
        "\t\tcov[1, 2] = cov[2, 1] = 0.0\n",
        "\t\tw0, w1, w2 = gmv(cov,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\t\t\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    \n",
        "\n",
        "\t\t# Equal-weighted portfolio\n",
        "\t\tmodel = 'est_none'\n",
        "\t\tcov[0, 0] = cov[1, 1] = cov[2, 2] = (sds.mean())**2\n",
        "\t\twgts = (1/n)*np.ones(n)\n",
        "\t\t# df.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_est_none\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   \n",
        "\n",
        "\n",
        "\n",
        "\tportret_list = ['raver_portret_' +  model for model in model_list]\n",
        "\tstats = df[portret_list].describe()\n",
        "\n",
        "\tsr_df = pd.DataFrame(dtype=float, columns = ['sr'], index = model_list)\n",
        "\tfor model in model_list:\n",
        "\t\tsr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]\n",
        "\t\t\n",
        "\treturn sr_df\n",
        "\n",
        "## Run for a systematic list of inputs (varying window length)\n",
        "# Took 1 hour to run 10 parms @ 250 sims each\n",
        "# Risk aversion\n",
        "raver = 2\n",
        "# Risk-free rate\n",
        "r = 0.02\n",
        "# Investment period\n",
        "T = 50\n",
        "\n",
        "# Number of simulations\n",
        "num_sims = 100\n",
        "\n",
        "# Asset Parameters\n",
        "mns1 = np.array([0.06, 0.10, 0.14])\n",
        "mns2 = np.array([0.08, 0.10, 0.12])\n",
        "\n",
        "sds1 = np.array([0.16, 0.20, 0.24])\n",
        "\n",
        "c1 = 0.75\n",
        "\n",
        "w1 = 10\n",
        "w2 = 20\n",
        "w3 = 30\n",
        "w4 = 40\n",
        "w5 = 50\n",
        "\n",
        "mns_dict = {'mns1':mns1, 'mns2':mns2}\n",
        "sds_dict = {'sds1':sds1}\n",
        "corr_dict= {'c1':c1}\n",
        "window_dict = {'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5 }\n",
        "\n",
        "iterables = [list(mns_dict.keys()),\n",
        "             list(sds_dict.keys()),\n",
        "             list(corr_dict.keys()), \n",
        "             list(window_dict.keys()),\n",
        "             np.arange(num_sims)]\n",
        "idx = pd.MultiIndex.from_product(iterables, names=[\"means\", \"sds\", \"corrs\", \"window\", \"sim\"])\n",
        "sim_results = pd.DataFrame(dtype='float', columns=['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd'], index=idx)\n",
        "\n",
        "for m in list(mns_dict.keys()):\n",
        "    means = mns_dict[m]\n",
        "    n = len(means)\n",
        "    for s in list(sds_dict.keys()):\n",
        "        sds = sds_dict[s]\n",
        "        for c in list(corr_dict.keys()):\n",
        "            corr12 = corr13 = corr23 = corr_dict[c]\n",
        "            # Covariance matrix\n",
        "            C  = np.identity(3)\n",
        "            C[0, 1] = C[1, 0] = corr12\n",
        "            C[0, 2] = C[2, 0] = corr13\n",
        "            C[1, 2] = C[2, 1] = corr23\n",
        "            cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "            for w in list(window_dict.keys()):\n",
        "\n",
        "                # print(m + \"\\t\" + s +  \"\\t\" + c + \"\\t\" + w)\n",
        "\n",
        "                # Run the simulations\n",
        "                for sim in range(num_sims):\n",
        "                    # if np.mod(sim,25)==0:\n",
        "                        # print('Simulation number: ' + str(sim))\n",
        "                    sim_results.loc[(m,s,c,w,sim)] = simulation(means, cov, short_lb=None, seed=sim, window=window_dict[w]).T.values\n",
        "\n",
        "stats = sim_results.groupby(['means', 'sds','corrs','window']).mean()\n",
        "stats = stats[['true','est_all', 'est_sd_corr', 'est_sd','est_none']]\n",
        "\n",
        "def compare_plot(mns,sds,corr):\n",
        "    newdf = stats.loc[(mns,sds,corr,slice(None))].stack().reset_index()\n",
        "    newdf.columns=['window','strategy','sr']\n",
        "    label_dict = {'true':'True',\n",
        "                'est_none': 'Est-None',\n",
        "                'est_all': 'Est-All',\n",
        "                'est_sd_corr': 'Est-SD-Corr',\n",
        "                'est_sd': 'Est-SD'}\n",
        "\n",
        "    newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])\n",
        "    newdf['window'] = newdf['window'].apply(lambda y: window_dict[y])\n",
        "    # newdf\n",
        "    import plotly.express as px\n",
        "    fig = go.Figure()\n",
        "    fig = px.histogram(newdf, x=\"strategy\", y=\"sr\",\n",
        "                color='window', barmode='group', histfunc='avg',\n",
        "                height=400)\n",
        "    fig.layout.yaxis[\"title\"] = \"Sharpe ratio\"\n",
        "    fig.layout.xaxis[\"title\"] = \"Strategy\"             \n",
        "    fig.show()\n",
        "\n",
        "compare_plot('mns1','sds1','c1')"
      ],
      "id": "fig-input-est-window",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With only 10 years of data, Est-All exhibits an average Sharpe ratio of about the same magnitude as Est-SD and Est-None.  With more data, however, Est-All performs better on average than the other strategies.  This need not always be the case.  For portfolios in which the true dispersion in expected returns is relatively small, as in @fig-input-est-window2, we may need much more data before estimating expected returns using historical means outperforms less ambitious estimation strategies.  For @fig-input-est-window2, even using 50 years of realized returns to estimate expected returns does not result in enough precision for Est-All to outperform Est-SD or Est-None, on average.\n"
      ],
      "id": "45206147"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-window2\n",
        "#| fig-cap: Simulated performance of various strategies as a function of input estimation window length--less dispersion in expected returns\n",
        "compare_plot('mns2','sds1','c1')"
      ],
      "id": "fig-input-est-window2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of Assets {-}\n",
        "\n",
        "Our ability to accurately estimate the set of inputs to portfolio optimization will also vary with the number of assets under consideration.  In particular, recall that the number of correlations scales quadratically with the number of assets.  We now consider the performance of the four estimation strategies for portfolios of 3, 5, and 10 assets.  We simulate 100 times for each case.  We use an estimation window of 30 years and an investment period of 50 years.   The covariance matrix is scaled such that the theoretical Sharpe ratio is the same for the 3, 5, and 10 asset portfolios.\n",
        "\n",
        "@fig-input-est-nassets reports the average realized Sharpe ratios across the simulations for each case. The average realized Sharpe ratio of each estimation strategy falls with the number of assets.  The reduction in performance is especially pronounced for Est-SD-Corr, which requires estimation of all pair-wise correlations.  For the 3 asset case, there are 3 correlations to estimate.  This increases to 10 correlations for the 5 asset porfolio and 45 for the 10-asset portfolio.  It is perhaps not surprising that we encounter worse performance when trying to estimate 45 different correlations.  Both Est-All and Est-SD-Corr estimate correlations. For the parameters in this example, estimating expected returns largely in Est-All undoes the poor performance due to estimating correlations found in Est-SD-Corr.\n"
      ],
      "id": "ac506d0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-nassets\n",
        "#| fig-cap: Simulated performance of various strategies as a function of the number of assets\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import pandas as pd\n",
        "\n",
        "# Risk aversion\n",
        "raver = 2\n",
        "\n",
        "# Risk-free rate\n",
        "r = 0.02\n",
        "\n",
        "num_sims = 100\n",
        "T = 50\n",
        "win=30\n",
        "\n",
        "\n",
        "# Simulation function\n",
        "def simulation(means, cov, short_lb, seed, window):\n",
        "\trets = mvn.rvs(means, cov, size=window+T, random_state = seed)\n",
        "\tn = len(means)\n",
        "\treturn_list = ['r' + str(i) for i in range(n)]\n",
        "\tmean_list  = ['mn' + str(i) for i in range(n)]\n",
        "\tsd_list  = ['sd' + str(i) for i in range(n)]\n",
        "\tcorr_list = ['c' + str(i) + str(j) for i in np.arange(n) for j in np.arange(i+1,n)]\n",
        "\twgt_list = ['wgt' + str(i) for i in range(n)]\n",
        "\tdf = pd.DataFrame(data=rets, columns=return_list)\n",
        "\n",
        "\t# Estimate rolling window historical inputs\n",
        "\tfor i in np.arange(n):\n",
        "\t\tdf[mean_list[i]] = df[return_list[i]].rolling(window).mean()\n",
        "\tfor i in np.arange(n):\n",
        "\t\tdf[sd_list[i]] = df[return_list[i]].rolling(window).std()\n",
        "\tcorrs = df[return_list].rolling(window, min_periods=window).corr()\n",
        "\tfor i in np.arange(n):\n",
        "\t\tfor j in np.arange(i+1,n):\n",
        "\t\t\tdf['c'+str(i)+str(j)]=corrs.loc[(slice(None),'r'+str(i)),'r'+str(j)].values\n",
        "    \n",
        "\twgts_true = tangency(means,cov,r,short_lb)\n",
        "\twgt_cal_true = (wgts_true @ means - r) / (raver * (wgts_true @ cov @ wgts_true))\n",
        "\n",
        "\n",
        "\tmodel_list = ['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd']\n",
        "\tfor model in model_list:\n",
        "\t\tdf['portret_'+model] = np.nan  # portret is the realized portfolio return of the 100% risky asset portfolio\n",
        "\t\tif model not in ['true','est_none']:\n",
        "\t\t\tfor wgt in wgt_list:\n",
        "\t\t\t\tdf[wgt+'_' + model] = np.nan\n",
        "\t\tdf['wgt_cal_'+model] =np.nan\n",
        "\t\tdf['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor\n",
        "\n",
        "\tfor i in np.arange(window,window+T):\n",
        "\t\t# Full estimation inputs at each point in time\n",
        "\t\tmeans = df[mean_list].iloc[i-1].values\n",
        "\t\tsds   = df[sd_list].iloc[i-1].values\n",
        "\t\tC = np.identity(n)\n",
        "\t\tfor i2 in np.arange(n):\n",
        "\t\t\tfor j in np.arange(i2+1,n):\n",
        "\t\t\t\tC[i2,j] = C[j,i2] = df.loc[i-1,'c'+str(i2)+str(j)]\n",
        "\t\tcov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "\t\t##### Note: all portfolio weights considered to be beginning of period weights\n",
        "\t\t##### (so multiply by contemporaneous realized returns)\n",
        "\t\t# Theoretical optimal weights\n",
        "\t\tmodel = 'true'\n",
        "\t\tdf.loc[i,'portret_'+model]= df.loc[i,return_list].values @ wgts_true\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + wgt_cal_true * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Full estimation tangency portfolio\n",
        "\t\tmodel = 'est_all'\n",
        "\t\twgts = tangency(means,cov,r,short_lb)\n",
        "\t\tfor a, wgt in enumerate(wgt_list):\n",
        "\t\t\tdf.loc[i,wgt+'_'+model] = wgts[a]\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Estimate only covariance matrix\n",
        "\t\tmodel = 'est_sd_corr'\n",
        "\t\twgts = gmv(cov,short_lb)\n",
        "\t\tfor a, wgt in enumerate(wgt_list):\n",
        "\t\t\tdf.loc[i,wgt+'_'+model] = wgts[a]\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\n",
        "\t\t# Estimate only standard deviations in covariance matrix\n",
        "\t\tmodel = 'est_sd'\n",
        "\t\tfor i2 in np.arange(n):\n",
        "\t\t\tfor j in np.arange(i2+1,n):\n",
        "\t\t\t\tcov[i2,j] = cov[j,i2] =0.0\t\t\n",
        "\t\twgts = gmv(cov,short_lb)\n",
        "\t\tfor a, wgt in enumerate(wgt_list):\n",
        "\t\t\tdf.loc[i,wgt+'_'+model] = wgts[a]\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\t\t\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    \n",
        "\n",
        "\t\t# Equal-weighted portfolio\n",
        "\t\tmodel = 'est_none'\n",
        "\t\tfor i2 in np.arange(n):\n",
        "\t\t\tcov[i2,i2] = (sds.mean())**2\n",
        "\t\twgts = (1/n)*np.ones(n)\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   \n",
        "\n",
        "\n",
        "\n",
        "\tportret_list = ['raver_portret_' +  model for model in model_list]\n",
        "\tstats = df[portret_list].describe()\n",
        "\n",
        "\tsr_df = pd.DataFrame(dtype=float, columns = ['sr'], index = model_list)\n",
        "\tfor model in model_list:\n",
        "\t\tsr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]\n",
        "\t\t\n",
        "\treturn sr_df\n",
        "\n",
        "# Make theoretical Sharpe ratio constant across cases\n",
        "mns3 = np.array([0.06, 0.10, 0.14])\n",
        "mns5 = np.array([0.06, 0.10, 0.14, 0.18, 0.22])\n",
        "mns10= np.array([0.06, 0.10, 0.14, 0.18, 0.22, 0.06, 0.10, 0.14, 0.18, 0.22])\n",
        "\n",
        "sds3 = np.array([0.16, 0.20, 0.24])\n",
        "sds5 = np.array([0.16, 0.20, 0.24, 0.28, 0.32])\n",
        "sds10= np.array([0.16, 0.20, 0.24, 0.28, 0.32, 0.16, 0.20, 0.24, 0.28, 0.32])\n",
        "\n",
        "corr = 0.5\n",
        "\n",
        "mns_dict = {'3':mns3, '5':mns5, '10':mns10}\n",
        "sds_dict = {'3':sds3, '5':sds5, '10':sds10}\n",
        "\n",
        "# Adjust covariance matrix so theoretical sharpe ratio is same\n",
        "sharpes = np.zeros(len(mns_dict.keys()))\n",
        "for k,key in enumerate(mns_dict.keys()):\n",
        "    means = mns_dict[key]\n",
        "    sds   = sds_dict[key]\n",
        "    n = len(means)\n",
        "    C = np.identity(n)\n",
        "    for i in np.arange(0,n):\n",
        "        for j in np.arange(i+1,n):\n",
        "            C[i,j] = C[j,i] = corr\n",
        "    cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "    wgts_true = tangency(means,cov,r,short_lb=None)\n",
        "    sr_true = (wgts_true @ means - r) / (np.sqrt(wgts_true @ cov @ wgts_true))\n",
        "    sharpes[k] = sr_true\n",
        "\n",
        "\n",
        "iterables = [list(mns_dict.keys()),\n",
        "             np.arange(num_sims)]\n",
        "idx = pd.MultiIndex.from_product(iterables, names=[\"n_assets\", \"sim\"])\n",
        "sim_results = pd.DataFrame(dtype='float', columns=['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd'], index=idx)\n",
        "\n",
        "\n",
        "for k,key in enumerate(mns_dict.keys()):\n",
        "    # print(key)\n",
        "    means = mns_dict[key]\n",
        "    sds   = sds_dict[key]\n",
        "    n = len(means)\n",
        "    C = np.identity(n)\n",
        "    for i in np.arange(0,n):\n",
        "        for j in np.arange(i+1,n):\n",
        "            C[i,j] = C[j,i] = corr\n",
        "    cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "    cov = cov * (sharpes[k]/sharpes[0])**2\n",
        "\n",
        "    # Run the simulations\n",
        "    for sim in range(num_sims):\n",
        "        # if np.mod(sim,25)==0:\n",
        "            # print('Simulation number: ' + str(sim))\n",
        "        sim_results.loc[(key,sim)] = simulation(means, cov, short_lb=None, seed=sim, window=win).T.values\n",
        "\n",
        "stats = sim_results.groupby(['n_assets']).mean()\n",
        "stats = stats.reset_index()\n",
        "stats['num'] = stats['n_assets'].apply(lambda x: int(x))\n",
        "stats = stats.sort_values('num')\n",
        "stats = stats[['num','true','est_all', 'est_sd_corr', 'est_sd','est_none']]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "import plotly.express as px\n",
        "newdf = stats.set_index('num').stack().reset_index()\n",
        "newdf.columns=['# of assets','strategy','sr']\n",
        "label_dict = {'true':'True',\n",
        "            'est_none': 'Est-None',\n",
        "            'est_all': 'Est-All',\n",
        "            'est_sd_corr': 'Est-SD-Corr',\n",
        "            'est_sd': 'Est-SD'}\n",
        "\n",
        "newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])\n",
        "fig = go.Figure()\n",
        "fig = px.histogram(newdf, x=\"strategy\", y=\"sr\",\n",
        "            color='# of assets', barmode='group', histfunc='avg',\n",
        "            height=400)\n",
        "fig.layout.yaxis[\"title\"] = \"Sharpe ratio\"\n",
        "fig.layout.xaxis[\"title\"] = \"Strategy\"             \n",
        "fig.show()\n"
      ],
      "id": "fig-input-est-nassets",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are unfortunately not hard-and-fast rules on when to try to estimate expected returns or correlations.  An investor must use their own judgement as to whether the underlying assets should exhibit large enough differences in expected returns to make it beneficial to try to differentiate between them on this dimension.  Similarly, for large portfolios, one must weight the potential diversification benefits of estimating correlations across assets with the potential costs of introducing estimation errors into a high-dimensional covariance matrix.  The benefits are probably more useful for optimization across asset classes than within asset class.  In later chapters, we will describe some methods for addressing both of the estimation issues described above.\n",
        "\n",
        "\n",
        "## Empirical Performance: Stocks, Bonds, and Gold\n",
        "\n",
        "We now examine how well we would have done forming optimal portfolios historically with the S&P 500, Treasury bonds, corporate bonds, and gold.  As in the previous section, we consider strategies differing in the extent to which they rely on estimates of expected returns, standard deviations, and correlations.  We use rolling windows of 20 years to estimate each input using historical data.  The data starts in 1968, so our first out-of-sample return is in 1988.  We rebalance the portfolio annually.  Each year, we locate our portfolio along the capital allocation line in oreder to match the expected return of an equal-weighted portfolio (which is the cross-sectional average of the historical average returns).\n",
        "\n",
        "@fig-stockbondsgold-sharpe shows the realized Sharpe ratio for each strategy.  Est-All underperformed each of the other strategies, including Est-None.  The noisiness in estimating expected returns does not lead to better investment results compared to any of the strategies that assume expected returns are equal across the assets.\n",
        "\n",
        "Est-SD (risk parity) performs the best in terms of Sharpe ratio in the thirty or so years of implementing each strategy, followed by the Est-SD-Corr and Est-None strategies.  For this set of assets and sample, estimating differences in volatility and correlations across assets was beneficial to performance.  \n"
      ],
      "id": "34856a34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import quandl\n",
        "quandl.ApiConfig.api_key = \"f-5zoU2G4zzHaUtkJ7BY\"\n",
        "\n",
        "\n",
        "def tangency(means, cov, rf, short_lb):\n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''\n",
        "\n",
        "    n = len(means)\n",
        "    def f(w):\n",
        "        mn = w @ means\n",
        "        sd = np.sqrt(w.T @ cov @ w)\n",
        "        return -(mn - rf) / sd\n",
        "    # Initial guess (equal-weighted)\n",
        "    w0 = (1/n)*np.ones(n)\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = np.ones(n)\n",
        "    b = 1\n",
        "    cons = [{\"type\": \"eq\", \"fun\": lambda x: A @ x - b}]\n",
        "    bnds = [(short_lb, None) for i in range(n)] \n",
        "    # Optimization\n",
        "    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x\n",
        "    return wgts_tangency\n",
        "def gmv(cov, short_lb): \n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''    \n",
        "    n = len(cov)\n",
        "    Q = matrix(cov, tc=\"d\")\n",
        "    p = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    if short_lb==None:\n",
        "        # No position limits\n",
        "        G = matrix(np.zeros((n,n)), tc=\"d\")\n",
        "        h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    else:\n",
        "        # Constraint: short-sales not allowed\n",
        "        G = matrix(-np.identity(n), tc=\"d\")\n",
        "        h = matrix(-short_lb * np.ones(n), (n, 1), tc=\"d\")\n",
        "    # Fully-invested constraint\n",
        "    A = matrix(np.ones(n), (1, n), tc=\"d\")\n",
        "    b = matrix([1], (1, 1), tc=\"d\")\n",
        "    sol = Solver(Q, p, G, h, A, b, options={'show_progress': False})\n",
        "    wgts_gmv = np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else np.array(n * [np.nan])\n",
        "    return wgts_gmv    \n",
        "\n",
        "def optimal(means, cov, raver, rs, rb, short_lb):\n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''  \n",
        "    n = len(cov)\n",
        "    Q = np.zeros((n + 2, n + 2))\n",
        "    Q[2:, 2:] = raver * cov\n",
        "    Q = matrix(Q, tc=\"d\")\n",
        "    p = np.array([-rs, -rb] + list(-means))\n",
        "    p = matrix(p, (n + 2, 1), tc=\"d\")\n",
        "    if short_lb==None:\n",
        "        # Constraint: short-sales unconstrained, saving weight positive, borrowing weight negative\n",
        "        G = np.zeros((2, n + 2))\n",
        "        G[0, 0] = -1\n",
        "        G[1, 1] = 1\n",
        "        G = matrix(G, (2, n+2), tc=\"d\")\n",
        "        h = matrix([0, 0], (2, 1), tc=\"d\")\n",
        "    else:    \n",
        "        # Constraint: short-sales constrained, saving weight positive, borrowing weight negative\n",
        "        G = -np.identity(n + 2)\n",
        "        G[1, 1] = 1\n",
        "        G = matrix(G, (n+2, n+2), tc=\"d\")\n",
        "        h = matrix(np.zeros(n+2), (n+2, 1), tc=\"d\")\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = matrix(np.ones(n+2), (1, n+2), tc=\"d\")\n",
        "    b = matrix([1], (1, 1), tc=\"d\")\n",
        "    sol = Solver(Q, p, G, h, A, b)\n",
        "    # function returns vector of risky asset weights\n",
        "    return np.array(sol[\"x\"]).flatten()[2:] if sol[\"status\"] == \"optimal\" else None    \n"
      ],
      "id": "8c329ed9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-stockbondsgold-sharpe\n",
        "#| fig-cap: Sharpe ratios of various strategies historically\n",
        "\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import pandas as pd\n",
        "\n",
        "# Pull the data (from sbb.py and gold.py from website codebase)\n",
        "# Stocks, bonds, bills\n",
        "nominal = pd.read_csv('https://www.dropbox.com/s/hgwte6swx57jqcv/nominal_sbb.csv?dl=1', index_col=['Year'])\n",
        "\n",
        "# Gold\n",
        "d = quandl.get(\"LBMA/GOLD\")['USD (AM)']\n",
        "gold = d.resample('Y').last().iloc[:-1]\n",
        "gold.index = [x.year for x in gold.index]\n",
        "gold.loc[1967] = d.iloc[0]\n",
        "gold = gold.sort_index().pct_change().dropna()\n",
        "gold.name = 'Gold'\n",
        " \n",
        "\n",
        "df = pd.concat((nominal, gold), axis=1).dropna()\n",
        "assets = ['TBills','S&P 500', 'Gold', 'Corporates', 'Treasuries']\n",
        "df = df[assets]\n",
        "\n",
        "##### Inputs\n",
        "# Window length (and initial period)\n",
        "window = 20\n",
        "n = len(assets)-1\n",
        "raver = 5\n",
        "short_lb = None\n",
        "T = len(df)-window\n",
        "\n",
        "\n",
        "\n",
        "# Rolling input estimation\n",
        "risky = assets[1:]\n",
        "df.columns = ['rf']+['r'+str(i) for i in range(n)]\n",
        "asset_list = [str(i) for i in range(n)]\n",
        "for asset in asset_list:\n",
        "    df['mn' + asset]=df['r'+asset].rolling(window).mean()\n",
        "    df['sd' + asset]=df['r'+asset].rolling(window).std()\n",
        "\n",
        "ret_list = ['r' + asset for asset in asset_list]\n",
        "corrs = df[ret_list].rolling(window, min_periods=window).corr()\n",
        "\n",
        "corr_list = []\n",
        "for j, asset in enumerate(asset_list):\n",
        "    for k in range(j+1,n):\n",
        "        df['c'+asset+str(k)]=corrs.loc[(slice(None),'r'+asset),'r'+str(k)].values\n",
        "df['year'] = df.index\n",
        "df = df.reset_index()\n",
        "\n",
        "\n",
        "# Prepare columns for the rolling optimization output\n",
        "model_list = ['ew', 'est_all', 'est_cov', 'est_sds']\n",
        "for model in model_list:\n",
        "    df['portret_'+model] = np.nan      #portret is the realized portfolio return of the 100% risky asset portfolio\n",
        "    if model not in ['ew']:\n",
        "        for asset in asset_list:\n",
        "            df['wgt' + asset + '_' +model] = np.nan\n",
        "    df['wgt_cal_'+model] =np.nan\n",
        "    df['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor\n",
        "\n",
        "mn_list = ['mn'+asset for asset in asset_list]\n",
        "sd_list = ['sd'+asset for asset in asset_list] \n",
        "\n",
        "# Choose optimal portfolios each time period\n",
        "for i in np.arange(window,window+T):\n",
        "    # Full estimation inputs at each point in time\n",
        "    means = df[mn_list].iloc[i-1].values\n",
        "    sds   = df[sd_list].iloc[i-1].values\n",
        "    C  = np.identity(n)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        for k in range(j+1,n):\n",
        "            C[j, k] = C[k, j] =    df.loc[i-1,'c'+asset+str(k)]  \n",
        "    cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "    r = df.loc[i,'rf']\n",
        "    ##### Note: all portfolio weights considered to be beginning of period weights\n",
        "    ##### (so multiply by contemporaneous realized returns)\n",
        "    # Full estimation tangency portfolio\n",
        "    model = 'est_all'\n",
        "    wgts = tangency(means,cov,r,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    # df.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))\n",
        "    # df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'wgt_cal_'+model] = (means.mean() - r)/ (wgts @ means - r)\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "    df.loc[i,'expret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (means @ wgts - r)\n",
        "    df.loc[i,'sd'+model] = df.loc[i,'wgt_cal_'+model] * np.sqrt(wgts @ cov @ wgts)\n",
        "\n",
        "    # Estimate only covariance matrix\n",
        "    model = 'est_cov'\n",
        "    wgts = gmv(cov,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    # df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "    # df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'wgt_cal_'+model] = 1.0\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "    df.loc[i,'expret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (means.mean() - r)\n",
        "    df.loc[i,'sd'+model] = df.loc[i,'wgt_cal_'+model] *np.sqrt(wgts @ cov @ wgts)\n",
        "\n",
        "    # Estimate only standard deviations in covariance matrix\n",
        "    model = 'est_sds'\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        for k in range(j+1,n):\n",
        "            cov[j, k] = cov[k, j] = 0.0\n",
        "    wgts = gmv(cov,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    # df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\t\t\n",
        "    # df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'wgt_cal_'+model] = 1.0\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    \n",
        "    df.loc[i,'expret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (means.mean() - r)\n",
        "    df.loc[i,'sd'+model] = df.loc[i,'wgt_cal_'+model] *np.sqrt(wgts @ cov @ wgts)\n",
        "\n",
        "    # Equal-weighted portfolio\n",
        "    model = 'ew'\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        cov[j,j] = (sds.mean())**2\n",
        "    wgts = (1/n)*np.ones(n)\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    # df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "    # df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'wgt_cal_'+model] = 1.0\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   \n",
        "    df.loc[i,'expret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (means.mean() - r)\n",
        "    df.loc[i,'sd'+model] = df.loc[i,'wgt_cal_'+model] *np.sqrt(wgts @ cov @ wgts)     \n",
        "\n",
        "# Summarize sharpe ratio, avg ret, sd(ret) for each model\n",
        "portret_list = ['raver_portret_' +  model for model in ['est_all', 'est_cov', 'est_sds','ew']]\n",
        "stats = df[portret_list].describe()\n",
        "sr_df = pd.DataFrame(dtype=float, columns = ['sr','avg_ret','sd_ret'], index = ['est_all', 'est_cov', 'est_sds','ew'])\n",
        "r = df[np.isnan(df['raver_portret_ew'])==False].rf.mean()\n",
        "for model in ['est_all', 'est_cov', 'est_sds','ew']:\n",
        "    sr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]\n",
        "    sr_df.loc[model,'avg_ret'] = stats.loc['mean','raver_portret_' +  model]\n",
        "    sr_df.loc[model,'sd_ret'] = stats.loc['std','raver_portret_' +  model]\n",
        "\n",
        "label_dict = {'true': 'theoretical optimal weights', \n",
        "            'ew': 'equal weights',\n",
        "            'est_all': 'estimate all inputs',\n",
        "            'est_cov': 'estimate covariance matrix only',\n",
        "            'est_sds': 'estimate standard deviations only'}\n",
        "\n",
        "xaxis_label_dict = {'true': 'theoretical optimal weights', \n",
        "            'ew': 'Est-None',\n",
        "            'est_all': 'Est-All',\n",
        "            'est_cov': 'Est-SD-Corr',\n",
        "            'est_sds': 'Est-SD'}\n",
        "sr_df = sr_df.reset_index()\n",
        "sr_df['label'] = sr_df['index'].apply(lambda x: label_dict[x])\n",
        "sr_df['xaxis_label'] = sr_df['index'].apply(lambda x: xaxis_label_dict[x])\n",
        "\n",
        "\n",
        "# Plot sharpe ratios\n",
        "string =\"Strategy: %{customdata[0]} <br>\"\n",
        "string += \"Sharpe ratio: %{y:0.3f}<br>\"\n",
        "string += \"Average return: %{customdata[1]:0.1%}<br>\"\n",
        "string += \"SD(return): %{customdata[2]:0.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=sr_df['xaxis_label'], y=sr_df['sr'], customdata=sr_df[['label','avg_ret','sd_ret']], hovertemplate=string))\n",
        "fig.layout.yaxis[\"title\"] = \"Sharpe ratio\"\n",
        "fig.layout.xaxis[\"title\"] = \"Strategy\"\n",
        "fig.show()"
      ],
      "id": "fig-stockbondsgold-sharpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@fig-stockbondsgold-timeseries plots the realized returns for each strategy.  The hoverdata contains the optimal risky portfolio weights as well as the fraction of capital allocated to the risky portfolio.  Since we match the expected return for the Est-None portfolio, only Est-All has time-varying allocation to the optimal all-risky portfolio since it is allows for different expected returns across assets.\n"
      ],
      "id": "f4c7ded8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-stockbondsgold-timeseries\n",
        "#| fig-cap: Returns and weights of various strategies historically\n",
        "# Plot the time-series of returns and portfolio weights.\n",
        "for asset in asset_list:\n",
        "    df['wgt'+asset + '_ew'] = 1/n\n",
        "       \n",
        "fig = go.Figure()\n",
        "for model in ['est_all', 'est_cov', 'est_sds', 'ew']:\n",
        "    string =  \"Strategy: \" + label_dict[model] +\" <br>\"\n",
        "    string += \"Year: %{x:4.0f}<br>\"\n",
        "    string += \"Return: %{y:0.1%}<br>\"\n",
        "    string += \"Weight in Risky Portfolio: %{customdata[0]: 0.1%} <br>\"\n",
        "    string += \"Risky Portfolio Weights:<br>\"\n",
        "    string += \"  \"+ risky[0] +\": %{customdata[1]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[1] +\": %{customdata[2]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[2] +\": %{customdata[3]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[3] +\": %{customdata[4]: 0.1%} <br>\"\n",
        "    string += \"<extra></extra>\"\n",
        "\n",
        "    wgt_list = ['wgt_cal_'+ model] + ['wgt'+asset + \"_\" + model for asset in asset_list]\n",
        "    trace=go.Scatter(x=df['year'], y=df['raver_portret_'+model], customdata=df[wgt_list], hovertemplate=string, name = xaxis_label_dict[model])\n",
        "    fig.add_trace(trace)\n",
        "fig.layout.yaxis[\"title\"] = \"Return\"\n",
        "fig.layout.xaxis[\"title\"] = \"Year\"\n",
        "fig.update_yaxes(tickformat=\".0%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "fig.update_xaxes(range=[df['year'].iloc[window], df.year.max()])\n",
        "fig.show()"
      ],
      "id": "fig-stockbondsgold-timeseries",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empirical Performance: Industry Portfolios\n"
      ],
      "id": "fd605488"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# Define portfolio class\n",
        "class portfolio:\n",
        "\n",
        "    def __init__(self, means, cov, Shorts):\n",
        "        self.means = np.array(means)\n",
        "        self.cov = np.array(cov)\n",
        "        self.Shorts = Shorts\n",
        "        self.n = len(means)\n",
        "        if Shorts:\n",
        "            w = np.linalg.solve(cov, np.ones(self.n))\n",
        "            self.GMV = w / np.sum(w)\n",
        "            w = np.linalg.solve(cov, means)\n",
        "            self.piMu = w / np.sum(w)\n",
        "        else:\n",
        "            n = self.n\n",
        "            Q = matrix(cov, tc=\"d\")\n",
        "            p = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "            G = matrix(-np.identity(n), tc=\"d\")\n",
        "            h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "            A = matrix(np.ones(n), (1, n), tc=\"d\")\n",
        "            b = matrix([1], (1, 1), tc=\"d\")\n",
        "            sol = Solver(Q, p, G, h, A, b)\n",
        "            self.GMV = np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else np.array(n * [np.nan])\n",
        "\n",
        "    def frontier(self, m):\n",
        "        if self.Shorts:\n",
        "            gmv = self.GMV\n",
        "            piMu = self.piMu\n",
        "            m1 = gmv @ self.means\n",
        "            m2 = piMu @ self.means\n",
        "            a = (m - m2) / (m1 - m2)\n",
        "            return a * gmv + (1 - a) * piMu\n",
        "        else:\n",
        "            n = self.n\n",
        "            Q = matrix(self.cov, tc=\"d\")\n",
        "            p = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "            G = matrix(-np.identity(n), tc=\"d\")\n",
        "            h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "            A = matrix(np.vstack((np.ones(n), self.means)), (2, n), tc=\"d\")\n",
        "            b = matrix([1, m], (2, 1), tc=\"d\")\n",
        "            sol = Solver(Q, p, G, h, A, b)\n",
        "            return np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else np.array(n * [np.nan])\n",
        "\n",
        "    def tangency(self, r):\n",
        "        if self.Shorts:\n",
        "            w = np.linalg.solve(self.cov, self.means - r)\n",
        "            return w / np.sum(w)\n",
        "        else:\n",
        "            def f(m):\n",
        "                w = self.frontier(m)\n",
        "                mn = w @ self.means\n",
        "                sd = np.sqrt(w.T @ self.cov @ w)\n",
        "                return - (mn - r) / sd\n",
        "            m = minimize_scalar(f, bounds=[max(r, np.min(self.means)), max(r, np.max(self.means))], method=\"bounded\").x\n",
        "            return self.frontier(m)\n",
        "\n",
        "    def optimal(self, raver, rs=None, rb=None):\n",
        "        n = self.n\n",
        "        if self.Shorts:\n",
        "            if (rs or rs==0) and (rb or rb==0):\n",
        "                Q = np.zeros((n + 2, n + 2))\n",
        "                Q[2:, 2:] = raver * self.cov\n",
        "                Q = matrix(Q, tc=\"d\")\n",
        "                p = np.array([-rs, rb] + list(-self.means))\n",
        "                p = matrix(p, (n + 2, 1), tc=\"d\")\n",
        "                G = np.zeros((2, n + 2))\n",
        "                G[0, 0] = G[1, 1] = -1\n",
        "                G = matrix(G, (2, n+2), tc=\"d\")\n",
        "                h = matrix([0, 0], (2, 1), tc=\"d\")\n",
        "                A = matrix([1, -1] + n*[1], (1, n+2), tc=\"d\")\n",
        "                b = matrix([1], (1, 1), tc=\"d\")\n",
        "                sol = Solver(Q, p, G, h, A, b)\n",
        "                return np.array(sol[\"x\"]).flatten()[2:] if sol[\"status\"] == \"optimal\" else None\n",
        "            else:\n",
        "                w = np.linalg.solve(self.cov, self.means)\n",
        "                a = np.sum(w)\n",
        "                return (a/raver)*self.piMu + (1-a/raver)*self.GMV\n",
        "        else:\n",
        "           if (rs or rs==0) and (rb or rb==0):\n",
        "                Q = np.zeros((n + 2, n + 2))\n",
        "                Q[2:, 2:] = raver * self.cov\n",
        "                Q = matrix(Q, tc=\"d\")\n",
        "                p = np.array([-rs, rb] + list(-self.means))\n",
        "                p = matrix(p, (n+2, 1), tc=\"d\")\n",
        "                G = matrix(-np.identity(n + 2), tc=\"d\")\n",
        "                h = matrix(np.zeros(n+2), (n+2, 1), tc=\"d\")\n",
        "                A = matrix([1, -1] + n * [1], (1, n+2), tc=\"d\")\n",
        "                b = matrix([1], (1, 1), tc=\"d\")\n",
        "                sol = Solver(Q, p, G, h, A, b)\n",
        "                return np.array(sol[\"x\"]).flatten()[2:] if sol[\"status\"] == \"optimal\" else None\n",
        "           else:\n",
        "                Q = matrix(raver * self.cov, tc=\"d\")\n",
        "                p = matrix(-self.means, (n, 1), tc=\"d\")\n",
        "                G = matrix(-np.identity(n), tc=\"d\")\n",
        "                h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "                A = matrix(np.ones(n), (1, n), tc=\"d\")\n",
        "                b = matrix([1], (1, 1), tc=\"d\")\n",
        "                sol = Solver(Q, p, G, h, A, b)\n",
        "                return np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else None"
      ],
      "id": "4254bce2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@sec-optimal-portfolios-practice discussed how allowing short sales could result in improvements in mean-variance efficiency.  One example of this was industry portfolios, in which the frontier with short-sales lay entirely to the left of the frontier without shorting in expected return-standard deviation space.  The discussion above, however, suggests that some or all of these efficiency gains may be challenging to obtain out-of-sample given the large number of parameters one needs to estimate for 48 industry portfolios.\n",
        "\n",
        "To assess this, we run the following exercise.  Using data from 1970 to 2022, we estimate historical average returns, standard deviations, and correlations for 48 industry portfolios each month.  We start in forming portfolios in 1980 in order to have an initial estimation of 120 months of data.  We use expanding windows to estimate inputs.  Based on the estimated inputs each month, we consider the optimal portfolio for a mean-variance investor with risk aversion of 3 that either is or is not short-sale constrained.\n",
        "\n",
        "@fig-industry-ssc-oos plots the resulting time series of realized returns.  It is clear that when shorting is allowed, the return series is much more volatile.  Less clear from the plot is that the average return is also higher for the portfolio with shorting.  However, the increase in average return is not enough to offset the higher volatility.  The realized Sharpe ratio of the portfolio without short-selling is higher than that of the portfolio with short-selling (0.1573 versus 0.1244, respectively).  Thus, the possible efficiency gains of allowing short-selling would not have materialized for portfolios of industries when historical estimates are used for expected returns, standard deviations, and correlations.\n"
      ],
      "id": "4c6d9a76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-industry-ssc-oos\n",
        "#| fig-cap: Industry Portfolios\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas_datareader import DataReader as pdr\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "SolverOptions[\"show_progress\"] = False\n",
        "\n",
        "# Read data and clean-up missing data (coded -99.99)\n",
        "ff48 = pdr(\"48_Industry_Portfolios\", \"famafrench\", start=1900)[0]\n",
        "\n",
        "# Clean-up missings\n",
        "for c in ff48.columns:\n",
        "    ff48[c] = np.where(ff48[c]==-99.99, np.nan, ff48[c])\n",
        "ff48 = ff48/100\n",
        "\n",
        "# Read factor data\n",
        "ff3 = pdr('F-F_Research_Data_Factors','famafrench', start=1900)[0]/100\n",
        "df = ff48.join(ff3['RF'])\n",
        "df = df.loc['1970-01':].copy()  # There is missing data prior to 1970\n",
        "\n",
        "# Estimation window\n",
        "WINDOW = 120\n",
        "# Risk aversion \n",
        "RAVER = 3\n",
        "\n",
        "\n",
        "asset_list = ff48.columns\n",
        "dates = df.index[WINDOW:]\n",
        "df['retp_ss'] = np.nan\n",
        "df['retp_noss'] = np.nan\n",
        "df['retp_ew'] = np.nan\n",
        "df['sum_risky_ss'] = np.nan\n",
        "df['sum_risky_noss'] = np.nan\n",
        "df['sum_risky_ew'] = np.nan\n",
        "\n",
        "# DataFrame for tracking industry weights\n",
        "df_wgts = pd.DataFrame(dtype=float, columns = pd.MultiIndex.from_product([['ss','noss','ew'], asset_list]), index=dates)\n",
        "\n",
        "\n",
        "#The following uses an expanding window to estimate parameters\n",
        "for d in dates:\n",
        "    data= df.loc[:d-1,asset_list] \n",
        "    mns = data.mean()\n",
        "    sds = data.std()\n",
        "    C   = data.corr()\n",
        "    cov = np.diag(sds)@ C @ np.diag(sds)\n",
        "    n = len(mns)\n",
        "    rf = df.loc[d,'RF']\n",
        "    # print(f'Date: {d}\\n')\n",
        "\n",
        "    # Optimal portfolios w/ shorts\n",
        "    ss  = portfolio(mns.values,cov.values, Shorts = True)\n",
        "    wgt_ss = ss.optimal(RAVER, rf, rf)\n",
        "    if wgt_ss.sum()<0:\n",
        "        wgt_ss = 0.0*wgt_ss\n",
        "    df.loc[d,'sum_risky_ss'] = wgt_ss.sum()\n",
        "    df.loc[d,'retp_ss'] = rf + wgt_ss @ (df.loc[d,asset_list] - rf)\n",
        "    df_wgts.loc[d,('ss',slice(None))] = wgt_ss\n",
        "    # print(f'\\n\\tWith Short-sales:')\n",
        "    # print(f'\\n\\tTotal Risky Weight: {wgt_ss.sum(): .2f}\\n\\tMin Weight: {wgt_ss.min(): .2f}\\n\\tMax Weight: {wgt_ss.max(): .2f}')\n",
        "\n",
        "    # Optimal portfolios w/o shorts\n",
        "    noss  = portfolio(mns.values,cov.values, Shorts = False)\n",
        "    wgt_noss = noss.optimal(RAVER, rf, rf)\n",
        "    if wgt_noss.sum()<0:\n",
        "        wgt_noss = 0.0*wgt_noss    \n",
        "    df.loc[d,'sum_risky_noss'] = wgt_noss.sum()        \n",
        "    df.loc[d,'retp_noss'] = rf + wgt_noss @ (df.loc[d,asset_list] - rf)\n",
        "    df_wgts.loc[d,('noss',slice(None))] = wgt_noss\n",
        "    # print(f'\\n\\tWithout Short-sales:')\n",
        "    # print(f'\\n\\tTotal Risky Weight: {wgt_noss.sum(): .2f}\\n\\tMin Weight: {wgt_noss.min(): .2f}\\n\\tMax Weight: {wgt_noss.max(): .2f}')    \n",
        "\n",
        "    # Equal-weighted\n",
        "    mns = mns.mean()*np.ones(n)\n",
        "    sds = sds.mean()*np.ones(n)\n",
        "    cov = np.diag(sds) @ np.identity(n) @ np.diag(sds)\n",
        "    ew = portfolio(mns, cov, Shorts=True)\n",
        "    wgt_ew = ew.optimal(RAVER,rf,rf)\n",
        "    if wgt_ew.sum()<0:\n",
        "        wgt_ew = 0.0*wgt_ew    \n",
        "    df.loc[d,'retp_ew'] = rf + wgt_ew @ (df.loc[d,asset_list] - rf)\n",
        "    df.loc[d,'sum_risky_ew'] = wgt_ew.sum() \n",
        "    df_wgts.loc[d,('ew',slice(None))] = wgt_ew       \n",
        "    # print(f'\\n\\tEqual-weighted:')\n",
        "    # print(f'\\n\\tTotal Risky Weight: {wgt_ew.sum(): .2f}\\n\\tMin Weight: {wgt_ew.min(): .2f}\\n\\tMax Weight: {wgt_ew.max(): .2f}')    \n",
        "\n",
        "portrets = df[['retp_ss','retp_noss','retp_ew','RF']+['sum_risky_' + v for v in ['ss','noss','ew']]]\n",
        "portrets = portrets[portrets.retp_ss.isnull()==False]\n",
        "portrets['Date'] = portrets.index.to_timestamp('M')\n",
        "\n",
        "# Plot the time-series of returns (sum of risky portfolio weights as hoverdata)\n",
        "fig = go.Figure()\n",
        "\n",
        "label_dict = {'ss': 'With short sales',\n",
        "            'noss': 'Without short sales'}        \n",
        "\n",
        "for model in ['ss', 'noss']:\n",
        "    string =  \"Strategy: \" + label_dict[model] +\" <br>\"\n",
        "    string += \"Date: %{x}<br>\"\n",
        "    string += \"Return: %{y:0.1%}<br>\"\n",
        "    string += \"Total Weight in Risky Portfolio: %{customdata: 0.2%} <br>\"\n",
        "    string += \"<extra></extra>\"\n",
        "\n",
        "    wgt_list = ['wgt_cal_'+ model] + ['wgt'+asset + \"_\" + model for asset in asset_list]\n",
        "    trace=go.Scatter(x=portrets['Date'], y=portrets['retp_'+model], customdata=portrets['sum_risky_'+model], \n",
        "        hovertemplate=string, name = label_dict[model])\n",
        "    fig.add_trace(trace)\n",
        "fig.layout.yaxis[\"title\"] = \"Return\"\n",
        "fig.layout.xaxis[\"title\"] = \"Date\"\n",
        "fig.update_yaxes(tickformat=\".0%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "fig.show()"
      ],
      "id": "fig-industry-ssc-oos",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}