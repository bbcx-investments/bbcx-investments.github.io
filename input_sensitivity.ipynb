{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Portfolio Optimization Sensitivity to Inputs\n",
        "\n",
        "Optimal risky portfolios can be quite sensitive to the inputs, which consist of expected returns, standard deviations, and correlations across assets.  An example is provided in the first section below.  In practice, one must estimate these inputs.  Expected returns are notoriously hard to estimate using historical data, which we discuss in detail in SECTION LINK, so we often resort to models for inputs here or assume expected returns are equal across assets.  Asset volatilities and correlations can be forecasted more sucessfully.  Forecasting correlations in portfolios with many assets is complicated by the fact that the number of correlations we need to estimate rises quadratically with the number of assets.\n",
        "\n",
        "## Sensitivity of weights\n",
        "\n",
        "Let's return to the example of investing in US equity, developed international, and emerging market funds from the portfolios chapter.  We will fix the input parameters for all but the US equity portfolio and see what happens to the tangency portfolio weights as we vary our assumptions about the US equity portfolio.\n",
        "\n",
        "### Expected returns {-}\n",
        "\n",
        "Portfolio optimization is quite sensitive to expected returns.  This is rather unfortunate because financial economists and practitioners are not particularly great at forecasting expected returns.  @fig-sensitivity-expret shows how the tangency portfolio weights changes as a function of different inputs for the US equity expected return.  With a drop in the expected return input of only about 0.5%, the allocation to US equity in the optimal risky portfolio goes to zero!  If the expected return input is 5%, the optimal portfolio has short exposure equal to about 50% of total capital.  Higher expected returns inputs result in greater allocations to the US equity portfolio.  If the expected return input is 7%, about 80% of the optimal portfolio is invested in US equities.\n"
      ],
      "id": "3f540305"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-expret\n",
        "#| fig-cap: Sensitivity of tangency weights to expected return input\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "##### Inputs\n",
        "# Risk-free rate\n",
        "r = 0.02\n",
        "# Expected returns\n",
        "means = np.array([0.06, 0.065, 0.08])\n",
        "# Standard deviations\n",
        "sds = np.array([0.15, 0.165, 0.21])\n",
        "# Correlations\n",
        "corr12 = 0.75\n",
        "corr13 = 0.75\n",
        "corr23 = 0.75\n",
        "# Covariance matrix\n",
        "C  = np.identity(3)\n",
        "C[0, 1] = C[1, 0] = corr12\n",
        "C[0, 2] = C[2, 0] = corr13\n",
        "C[1, 2] = C[2, 1] = corr23\n",
        "cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "def tangency(means, cov, rf, Shorts):\n",
        "    n = len(means)\n",
        "    def f(w):\n",
        "        mn = w @ means\n",
        "        sd = np.sqrt(w.T @ cov @ w)\n",
        "        return -(mn - rf) / sd\n",
        "    # Initial guess (equal-weighted)\n",
        "    w0 = (1/n)*np.ones(n)\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = np.ones(n)\n",
        "    b = 1\n",
        "    cons = [{\"type\": \"eq\", \"fun\": lambda x: A @ x - b}]\n",
        "    if Shorts==True:\n",
        "        # No short-sale constraint\n",
        "        bnds = [(None, None) for i in range(n)] \n",
        "    else:\n",
        "        # With short-sale constraint\n",
        "        bnds = [(0, None) for i in range(n)] \n",
        "    # Optimization\n",
        "    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x\n",
        "    return wgts_tangency\n",
        "\n",
        "wgts_true = tangency(means,cov,r,Shorts=True)\n",
        "\n",
        "# Tangency portfolios for a range of assumed asset 1 expected returns\n",
        "n = len(means)\n",
        "num_grid=100\n",
        "asset1_means = np.linspace(0.04,0.10,num_grid)\n",
        "wgts = np.zeros((num_grid,n))\n",
        "\n",
        "for i,m in enumerate(asset1_means):\n",
        "    wgts[i] = tangency(np.array([m, means[1], means[2]]),cov,r,Shorts=True)\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Asset 1 Expected Return Input = %{x:0.2%}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=asset1_means,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assume input\n",
        "trace_true = go.Scatter(x=[means[0]],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Asset 1 Expected Return Input\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-expret",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Deviations {-}\n",
        "\n",
        "@fig-sensitivity-sd shows the sensitivity of the US market allocation in the tangency portfolio for different standard deviation inputs.  For values in the region of the historical average volatility of 15%, higher standard deviation inputs result in lower allocations to US equities.\n"
      ],
      "id": "ad2fb919"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-sd\n",
        "#| fig-cap: Sensitivity of tangency weights to standard deviation input\n",
        "##### Variance input of asset 1:\n",
        "# Tangency portfolios for a range of assumed asset 1 standard deviations\n",
        "asset1_sds = np.linspace(0.05,0.25,num_grid)\n",
        "wgts = np.zeros((num_grid,n))\n",
        "\n",
        "for i,s in enumerate(asset1_sds):\n",
        "    sds_new = np.array([s, sds[1], sds[2]])\n",
        "    wgts[i] = tangency(means,np.diag(sds_new) @ C @ np.diag(sds_new),r,Shorts=True)\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Asset 1 Standard Deviation Input = %{x}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=asset1_sds,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assumed input\n",
        "trace_true = go.Scatter(x=[sds[0]],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Asset 1 Standard Deviation Input\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.55))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-sd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlations {-}\n",
        "\n",
        "@fig-sensitivity-corr shows the sensitivity of the US market's weight in the tangency portfolio for different inputs of the correlation between the US equity fund and the developed international equity fund.  Lower correlations imply greater diversification benefits, so more capital is allocated to US equities for lower correlation levels, all else equal.\n"
      ],
      "id": "2b288db9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-sensitivity-corr\n",
        "#| fig-cap: Sensitivity of tangency weights to standard deviation input\n",
        "##### Correlation of assets 1 and 2:\n",
        "# Tangency portfolios for a range of assumed asset 1 standard deviations\n",
        "corr12_grid = np.linspace(0.15,0.95,num_grid)\n",
        "wgts = np.empty((num_grid,n))\n",
        "\n",
        "def is_pos_def(x):\n",
        "    if np.all(np.linalg.eigvals(x) > 0):\n",
        "        return 'True'\n",
        "    else:\n",
        "        return 'False'\n",
        "\n",
        "for i,c in enumerate(corr12_grid):\n",
        "    # Covariance matrix\n",
        "    C  = np.identity(3)\n",
        "    C[0, 1] = C[1, 0] = c\n",
        "    C[0, 2] = C[2, 0] = corr13\n",
        "    C[1, 2] = C[2, 1] = corr23\n",
        "    # Check feasible correlations\n",
        "    if is_pos_def(C):\n",
        "        wgts[i] = tangency(means,np.diag(sds) @ C @ np.diag(sds),r,Shorts=True)\n",
        "    else:\n",
        "        print(\"not positive definite\" + str(c*100))\n",
        "wgt_asset1 = wgts[:,0]\n",
        "cd = np.empty(shape=(num_grid, n-1,1), dtype=float)\n",
        "cd[:, 0] = wgts[:,1].reshape(-1, 1)\n",
        "cd[:, 1] = wgts[:,2].reshape(-1, 1)\n",
        "string = \"Input: Correlation of Assets 1 and 2 = %{x}<br>\"\n",
        "string +=\"Tangency Portfolio Weights:<br>\"\n",
        "string += \"  Asset 1: %{y:0.1%}<br>\"\n",
        "string += \"  Asset 2: %{customdata[0]:.1%}<br>\"\n",
        "string += \"  Asset 3: %{customdata[1]:.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "trace = go.Scatter(x=corr12_grid,y=wgt_asset1,mode='lines', name=\"Tangency Weight\",\n",
        "    customdata=cd, hovertemplate=string,\n",
        ")\n",
        "\n",
        "# Tangency portfolio at assumed input\n",
        "trace_true = go.Scatter(x=[corr12],y=[wgts_true[0]],mode='markers', name=\"Tangency Weight at Assumed Input\",\n",
        "    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,\n",
        ")\n",
        "fig = go.Figure()\n",
        "fig.add_trace(trace)\n",
        "fig.add_trace(trace_true)\n",
        "fig.layout.xaxis[\"title\"] = \"Correlation of Assets 1 and 2\"\n",
        "fig.layout.yaxis[\"title\"] = \"Asset 1 Tangency Portfolio Weight\"\n",
        "fig.update_yaxes(tickformat=\".1%\")\n",
        "fig.update_xaxes(tickformat=\".1%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.55))\n",
        "fig.show()"
      ],
      "id": "fig-sensitivity-corr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Maximization {-}\n",
        "\n",
        "The sensitivity of the tangency portfolio to each of these inputs presents a practical problem for investors seeking to implement optimal portfolios in a quantitative sense.  Portfolio optimization is sometimes referred to as error maximization because the tangency portfolio weights may reflect errors in the inputs rather than optimal diversification and risk-reward trade-offs.\n",
        "\n",
        "When the estimated expected return for an asset is too high relative to its true (but unknown) value, the resulting tangency portfolio will put too much weight on that asset, relative to the unknown *true* tangency portfolio.  Similarly, the tangency portfolio weight will be too low for an asset with an over-estimated variance relative to the tangency portfolio calculated using the true (but unknown) variance.  Finally, optimal portfolios will overallocate capital to assets with underestimated correlations and underallocate capital to assets with overestimated correlations.  The error maximization problem becomes more acute for portfolios with many assets simply because the increase in the number of correlations to be estimated increases the likelihood of substantial errors.\n",
        "\n",
        "## Managing the Input List\n",
        "\n",
        "A simple way to estimate expected return, standard deviations, and correlations is to use historical data.  Specifically, the historical arithmetic average return is an estimate of an asset's expected return.  The historical standard deviation is an estimate of the volatility of the asset's return.  Historical correlations across assets can be estimated using the historical return series of each pair of assets.\n",
        "\n",
        "To understand the estimation issues that arise in practice, we can simulate the performance of various optimization strategies for choosing portfolios of US, developed international, and emerging market equity funds.  We will simulate from an assumed distribution corresponding to the inputs we have been using so far and then use this simulated \"historical\" data to estimate inputs to our portfolio optimization problem each year.  We assume an investment horizon of 50 years.  Each year, we'll use the last 30 years of data to estimate the inputs and rebalance our portfolio to the optimal portfolio based on the new input estimates.  As a benchmark, we'll use a simple equal-weighted portfolio of the three funds.  For this example, an equal-weighted portfolio is fairly close to the \"perfect information\" tangency portfolio of 27.5% in US equities, 33% in developed international, and 39.5% in emerging markets.  Note that an equal-weighted portfolio is the optimal risky portfolio if we assume that (i) all expected returns are the same (ii) all standard deviations are the same, and (iii) all correlations across assets are the same.  Thus, the equal-weighted portfolio is the optimal portfolio if we \"give up\" on estimating differences in inputs across assets.\n",
        "\n",
        "### Estimating Expected Returns, Standard Deviations, and Correlations {-}\n",
        "\n",
        "We run 500 simulations of the exercise above.  The empirical Sharpe ratios of the equal-weighted strategy and the full historical estimation strategy are plotted in @fig-input-est-1, along with the 45-degree line.  The equal-weighted strategy outperformed the full estimation strategy in terms of realized Sharpe ratios for any data point falling below the 45-degree line.  This occurs in just over three-quarters of the simulations.  Trying to estimate the nine inputs each period and find the optimal portfolio generally performed worse than simply equally-weighting the three funds.  @fig-input-est-histogram1 plots the histogram of the difference in simulated Sharpe ratios of the equal-weighted portfolio minus that of the full historical estimation strategy.  On average across the simulations, the Sharpe ratio of the EW strategy is 0.058 higher than the Sharpe ratio of the full historical estimation strategy.\n"
      ],
      "id": "b0310c4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-1\n",
        "#| fig-cap: Simulated performance of estimated tangency portfolio versus equal-weighted portfolio\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import pandas as pd\n",
        "##### Inputs\n",
        "# Number of simulations\n",
        "num_sims = 500\n",
        "\n",
        "# Risk aversion\n",
        "raver = 2\n",
        "\n",
        "# Risk-free rate\n",
        "r = 0.02\n",
        "# Expected returns\n",
        "means = np.array([0.06, 0.065, 0.08])\n",
        "# Standard deviations\n",
        "sds = np.array([0.15, 0.165, 0.21])\n",
        "\n",
        "# Correlations\n",
        "corr12 = 0.75\n",
        "corr13 = 0.75\n",
        "corr23 = 0.75\n",
        "# Covariance matrix\n",
        "C  = np.identity(3)\n",
        "C[0, 1] = C[1, 0] = corr12\n",
        "C[0, 2] = C[2, 0] = corr13\n",
        "C[1, 2] = C[2, 1] = corr23\n",
        "cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "# Window length (and initial period)\n",
        "window = 30\n",
        "\n",
        "# Length of out-of-sample period\n",
        "T = 50\n",
        "\n",
        "n = len(means)\n",
        "\n",
        "def tangency(means, cov, rf, short_lb):\n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''\n",
        "\n",
        "    n = len(means)\n",
        "    def f(w):\n",
        "        mn = w @ means\n",
        "        sd = np.sqrt(w.T @ cov @ w)\n",
        "        return -(mn - rf) / sd\n",
        "    # Initial guess (equal-weighted)\n",
        "    w0 = (1/n)*np.ones(n)\n",
        "    # Constraint: fully-invested portfolio\n",
        "    A = np.ones(n)\n",
        "    b = 1\n",
        "    cons = [{\"type\": \"eq\", \"fun\": lambda x: A @ x - b}]\n",
        "    bnds = [(short_lb, None) for i in range(n)] \n",
        "    # Optimization\n",
        "    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x\n",
        "    return wgts_tangency\n",
        "\n",
        "\n",
        "\n",
        "def gmv(cov, short_lb): \n",
        "    '''\n",
        "    short_lb: lower bound on position weights\n",
        "    examples: 0  = no short-selling\n",
        "              -1 = no more than -100% in a given asset\n",
        "              None=no restrictions on short-selling\n",
        "    '''    \n",
        "    n = len(cov)\n",
        "    Q = matrix(cov, tc=\"d\")\n",
        "    p = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    if short_lb==None:\n",
        "        # No position limits\n",
        "        G = matrix(np.zeros((n,n)), tc=\"d\")\n",
        "        h = matrix(np.zeros(n), (n, 1), tc=\"d\")\n",
        "    else:\n",
        "        # Constraint: short-sales not allowed\n",
        "        G = matrix(-np.identity(n), tc=\"d\")\n",
        "        h = matrix(-short_lb * np.ones(n), (n, 1), tc=\"d\")\n",
        "    # Fully-invested constraint\n",
        "    A = matrix(np.ones(n), (1, n), tc=\"d\")\n",
        "    b = matrix([1], (1, 1), tc=\"d\")\n",
        "    sol = Solver(Q, p, G, h, A, b, options={'show_progress': False})\n",
        "    wgts_gmv = np.array(sol[\"x\"]).flatten() if sol[\"status\"] == \"optimal\" else np.array(n * [np.nan])\n",
        "    return wgts_gmv\n",
        "\n",
        "# wgts_gmv = gmv(cov,short_lb=None)\n",
        "\n",
        "\n",
        "# Simulate data\n",
        "def simulation(means, cov, short_lb, seed):\n",
        "\trets = mvn.rvs(means, cov, size=window+T, random_state = seed)\n",
        "\tdf = pd.DataFrame(data=rets, columns=['r0','r1','r2'])\n",
        "\tdf.columns\n",
        "\tdf['mn0']=df['r0'].rolling(window).mean()\n",
        "\tdf['mn1']=df['r1'].rolling(window).mean()\n",
        "\tdf['mn2']=df['r2'].rolling(window).mean()\n",
        "\tdf['sd0']=df['r0'].rolling(window).std()\n",
        "\tdf['sd1']=df['r1'].rolling(window).std()\n",
        "\tdf['sd2']=df['r2'].rolling(window).std()\n",
        "\n",
        "\tcorrs = df[['r0','r1','r2']].rolling(window, min_periods=window).corr()\n",
        "\tdf['c01']=corrs.loc[(slice(None),'r0'),'r1'].values\n",
        "\tdf['c02']=corrs.loc[(slice(None),'r0'),'r2'].values\n",
        "\tdf['c12']=corrs.loc[(slice(None),'r1'),'r2'].values\n",
        "    \n",
        "\twgts_true = tangency(means,cov,r,short_lb)\n",
        "\twgt_cal_true = (wgts_true @ means - r) / (raver * (wgts_true @ cov @ wgts_true))\n",
        "\n",
        "\n",
        "\tmodel_list = ['true', 'ew', 'est_all', 'est_cov', 'est_sds']\n",
        "\tfor model in model_list:\n",
        "\t\tdf['portret_'+model] = np.nan  # portret is the realized portfolio return of the 100% risky asset portfolio\n",
        "\t\tif model not in ['true','ew']:\n",
        "\t\t\tdf['wgt0_'+model] = np.nan\n",
        "\t\t\tdf['wgt1_'+model] = np.nan\n",
        "\t\t\tdf['wgt2_'+model] = np.nan\n",
        "\t\tdf['wgt_cal_'+model] =np.nan\n",
        "\t\tdf['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor\n",
        "\n",
        "\tfor i in np.arange(window,window+T):\n",
        "\t\t# Full estimation inputs at each point in time\n",
        "\t\tmeans = df[['mn0','mn1','mn2']].iloc[i-1].values\n",
        "\t\tsds   = df[['sd0','sd1','sd2']].iloc[i-1].values\n",
        "\t\tcorr01 = df.loc[i-1,'c01']\n",
        "\t\tcorr02 = df.loc[i-1,'c02']\n",
        "\t\tcorr12 = df.loc[i-1,'c12']\n",
        "\t\tC  = np.identity(3)\n",
        "\t\tC[0, 1] = C[1, 0] = corr01\n",
        "\t\tC[0, 2] = C[2, 0] = corr02\n",
        "\t\tC[1, 2] = C[2, 1] = corr12\n",
        "\t\tcov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "\t\t##### Note: all portfolio weights considered to be beginning of period weights\n",
        "\t\t##### (so multiply by contemporaneous realized returns)\n",
        "\t\t# Theoretical optimal weights\n",
        "\t\tmodel = 'true'\n",
        "\t\tdf.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_true\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + wgt_cal_true * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Full estimation tangency portfolio\n",
        "\t\tmodel = 'est_all'\n",
        "\t\tw0, w1, w2 = tangency(means,cov,r,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\t\t# Estimate only covariance matrix\n",
        "\t\tmodel = 'est_cov'\n",
        "\t\tw0, w1, w2 = gmv(cov,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\n",
        "\t\t# Estimate only standard deviations in covariance matrix\n",
        "\t\tmodel = 'est_sds'\n",
        "\t\tcov[0, 1] = cov[1, 0] = 0.0\n",
        "\t\tcov[0, 2] = cov[2, 0] = 0.0\n",
        "\t\tcov[1, 2] = cov[2, 1] = 0.0\n",
        "\t\tw0, w1, w2 = gmv(cov,short_lb)\n",
        "\t\tdf.loc[i,'wgt0_' + model] = w0\n",
        "\t\tdf.loc[i,'wgt1_' + model] = w1\n",
        "\t\tdf.loc[i,'wgt2_' + model] = w2\n",
        "\t\t# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values\n",
        "\t\twgts = np.array([w0, w1, w2])\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\t\t\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    \n",
        "\n",
        "\t\t# Equal-weighted portfolio\n",
        "\t\tmodel = 'ew'\n",
        "\t\tcov[0, 0] = cov[1, 1] = cov[2, 2] = (sds.mean())**2\n",
        "\t\twgts = (1/n)*np.ones(n)\n",
        "\t\t# df.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_ew\n",
        "\t\tdf.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "\t\tdf.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "\t\tdf.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   \n",
        "\n",
        "\n",
        "\n",
        "\tportret_list = ['raver_portret_' +  model for model in model_list]\n",
        "\tstats = df[portret_list].describe()\n",
        "\n",
        "\tsr_df = pd.DataFrame(dtype=float, columns = ['sr'], index = model_list)\n",
        "\tfor model in model_list:\n",
        "\t\tsr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]\n",
        "\t\t\n",
        "\treturn sr_df\n",
        "\n",
        "sim_results = pd.DataFrame(dtype=float, columns=['true', 'ew', 'est_all', 'est_cov', 'est_sds'], index=range(num_sims))\n",
        "for s in range(num_sims):\n",
        "    # print('Simulation number: ' + str(s))\n",
        "    sim_results.iloc[s] = simulation(means, cov, short_lb=None, seed=s).T\n",
        "\n",
        "def figplot(xvar, yvar):\n",
        "    label_dict = {'true': 'theoretical optimal weights', \n",
        "                'ew': 'equal weights',\n",
        "                'est_all': 'estimate all inputs',\n",
        "                'est_cov': 'estimate covariance matrix only',\n",
        "                'est_sds': 'estimate standard deviations only'}\n",
        "\n",
        "\n",
        "    # Plot simulated Sharpe ratios\n",
        "    string =\"Sharpe Ratios:<br>\"\n",
        "    string += \"  \"+ label_dict[xvar] + \": %{x:0.3}<br>\"\n",
        "    string += \"  \"+ label_dict[yvar] + \": %{y:0.3}<br>\"\n",
        "    string += \"<extra></extra>\"\n",
        "\n",
        "    trace = go.Scatter(x=sim_results[xvar],y=sim_results[yvar],mode='markers', name='A simulated outcome', hovertemplate=string)\n",
        "    max_sr = sim_results.max().max()\n",
        "\n",
        "    # Plot 45 degree line\n",
        "    frac_x_beats_y = (sim_results[xvar] > sim_results[yvar]).mean()\n",
        "\n",
        "    string =\"Below this line: \" + f'({frac_x_beats_y:.0%} of simulations)<br>' + \"\"\n",
        "    string +=\"   `\"+label_dict[xvar] + \"` outperformed `\" +label_dict[yvar] + \"` \" + \"<br>\"\n",
        "    string +=\"Above this line: \" + f'({1-frac_x_beats_y:.0%} of simulations)<br>' + \"\" \n",
        "    string +=\"   `\"+label_dict[yvar] + \"` outperformed `\" +label_dict[xvar] + \"`<br>\"    \n",
        "    string += \"<extra></extra>\"   \n",
        "    trace_45 = go.Scatter(x=np.linspace(0,max_sr,100),y=np.linspace(0,max_sr,100),mode='lines', name='45-degree line', hovertemplate=string)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(trace)\n",
        "    fig.add_trace(trace_45)\n",
        "    fig.layout.xaxis[\"title\"] = \"SR: \" + label_dict[xvar]\n",
        "    fig.layout.yaxis[\"title\"] = \"SR: \" + label_dict[yvar]\n",
        "    fig.update_yaxes(tickformat=\".2\")\n",
        "    fig.update_xaxes(tickformat=\".2\")\n",
        "    fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "    fig.update_xaxes(range=[0, 1.05 * max_sr])\n",
        "    fig.update_yaxes(range=[0, 1.05 * max_sr])\n",
        "    fig.show()\n",
        "def histplot(xvar, yvar):\n",
        "    label_dict = {'true': 'theoretical optimal weights', \n",
        "                'ew': 'equal weights',\n",
        "                'est_all': 'estimate all inputs',\n",
        "                'est_cov': 'estimate covariance matrix only',\n",
        "                'est_sds': 'estimate standard deviations only'}\n",
        "\n",
        "    # Plot differences in Sharpe ratios\n",
        "    avg_diff = (sim_results[xvar] - sim_results[yvar]).mean()\n",
        "    frac_x_beats_y = (sim_results[xvar] > sim_results[yvar]).mean()\n",
        "    string = \"This bin contains %{y:0.3}% of simulations <br>\"\n",
        "    string +=\"Average difference in SRs across all simulations: \" + f'{avg_diff:.3f} <br>'\n",
        "    string +=\"'\" + label_dict[xvar] + \"' outperforms '\" + label_dict[yvar] + \"' in \" +  f'{frac_x_beats_y:.0%} of all simulations<br>' + \"\" \n",
        "    string += \"<extra></extra>\"   \n",
        "    trace=go.Histogram(x=(sim_results[xvar] - sim_results[yvar]), hovertemplate = string )\n",
        "\n",
        "    max_sr = sim_results.max().max()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(trace)\n",
        "    fig.layout.xaxis[\"title\"] = \"Difference in SRs: '\" + label_dict[xvar] + \"' minus '\" + label_dict[yvar] + \"'\"\n",
        "    fig.layout.yaxis[\"title\"] = \"Percent of Simulations\"\n",
        "    fig.update_yaxes(tickformat=\".2\")\n",
        "    fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "    fig.update_xaxes(range=[-0.6*max_sr, 0.6*max_sr])\n",
        "    # fig.update_yaxes(range=[0, 1.05 * sim_results.max().max()])\n",
        "    fig.update_traces(marker_line_width=1,marker_line_color=\"black\", histnorm='percent')    \n",
        "    fig.show()\n",
        "\n",
        "figplot('ew','est_all')"
      ],
      "id": "fig-input-est-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-histogram1\n",
        "#| fig-cap: Differences in Sharpe Ratios of estimated tangency portfolio and equal-weighted portfolio\n",
        "#| echo: false\n",
        "histplot('ew','est_all')"
      ],
      "id": "fig-input-est-histogram1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimating Standard Deviations and Correlations Only {-}\n",
        "One option is to just give up on using historical data to estimate expected returns and assume the expected returns are the same across all the assets.  Under this assumption, the optimal risky asset is the global minimum variance portfolio.  @fig-input-est-2 shows the Sharpe ratios of following a GMV strategy each year calculated using historical estimates of standard deviations and correlations.  The benchmark along the horizontal axis is again the equal-weighted strategy.  Portfolio optimization fares better if we do not try to estimate expected returns.  The fraction of simulations in which the equal-weighted strategy beats the GMV strategy using the full covariance matrix estimate is again about three-quarters of the simulations.  @fig-input-est-histogram2 plots the histogram of the difference in simulated Sharpe ratios of the equal-weighted portfolio minus that of the GMV strategy.  On average across the simulations, the Sharpe ratio of the EW strategy is 0.048 higher than the Sharpe ratio of the GMV strategy.  Thus, the GMV portfolio performs a bit better relative to an equal-weighted portfolio than trying to estimate the covariance matrix **and** expected returns.  \n"
      ],
      "id": "4de18117"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-2\n",
        "#| fig-cap: Simulated performance of estimated GMV portfolio versus equal-weighted portfolio\n",
        "#| echo: false\n",
        "figplot('ew','est_cov')"
      ],
      "id": "fig-input-est-2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-histogram2\n",
        "#| fig-cap: Differences in Sharpe Ratios of estimated GMV portfolio and equal-weighted portfolio\n",
        "#| echo: false\n",
        "histplot('ew','est_cov')"
      ],
      "id": "fig-input-est-histogram2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimating Standard Deviations Only {-}\n",
        "\n",
        "The advantage of the equal-weighted strategy falls further if we also give up on trying to estimate correlations.  @fig-input-est-3 shows the Sharpe ratios of following a GMV strategy using a covariance matrix with historical estimates of variances but covariances set equal to zero.  This strategy is known as risk parity.  The equal-weighted strategy outperforms this strategy in almost two-thirds of the simulations, but the average difference in Sharpe ratios is economically small.  On average across the simulations, the Sharpe ratio of the EW strategy is only 0.003 higher than the Sharpe ratio of the risk parity strategy.  @fig-input-est-histogram3 plots the histogram of the difference in simulated Sharpe ratios of the equal-weighted portfolio minus that of the risk parity strategy.  Note that there is much less variation in the differences in these strategies than in the GMV or tangency portfolio cases above.  \n"
      ],
      "id": "0b2790b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-3\n",
        "#| fig-cap: Simulated performance of risk parity portfolio versus equal-weighted portfolio\n",
        "#| echo: false\n",
        "figplot('ew','est_sds')"
      ],
      "id": "fig-input-est-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-input-est-histogram3\n",
        "#| fig-cap: Differences in Sharpe Ratios of risk parity portfolio and equal-weighted portfolio\n",
        "#| echo: false\n",
        "histplot('ew','est_sds')"
      ],
      "id": "fig-input-est-histogram3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empirical Performance\n",
        "\n",
        "We now examine how well we would have done forming optimal portfolios historically with the S&P 500, Treasury bonds, corporate bonds, and gold.  As in the previous section, we consider strategies differing in the extent to which they rely on estimates of expected returns, standard deviations, and correlations.  We use rolling windows of 20 years to estimate each input using historical data.  The data starts in 1968, so our first out-of-sample return is in 1988.  We rebalance the portfolio annually.  We optimally locate along the capital allocation line for each strategy's optimal portfolio, assuming a risk aversion of 5.^[This is a somewhat high risk aversion.  With lower risk aversion choices, some strategies have single year return realizations below -100%.]\n",
        "\n",
        "@fig-stockbondsgold-sharpe shows the realized Sharpe ratio for each strategy.  The tangency portfolio strategy underperformed each of the other strategies, including the equal-weighted $1/N$ strategy.  The noisiness in estimating expected returns does not lead to better investment results compared to any of the strategies that assume expected returns are equal across the assets.\n",
        "\n",
        "The risk parity strategy performs the best in terms of Sharpe ratio in the thirty or so years of implementing each strategy, followed by the equal-weighted and GMV strategies.  For this set of assets and sample, estimating differences in volatility across assets was beneficial to performance.  However, estimating correlations in addition to volatilities led to worse outcomes in terms of Sharpe ratios.  In fact, the GMV strategy underperformed both risk parity and the equal-weighted $1/N$ strategies.\n"
      ],
      "id": "a1cc3274"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import quandl\n",
        "quandl.ApiConfig.api_key = \"f-5zoU2G4zzHaUtkJ7BY\""
      ],
      "id": "cdc10db7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-stockbondsgold-sharpe\n",
        "#| fig-cap: Sharpe ratios of various strategies historically\n",
        "\n",
        "import numpy as np\n",
        "from cvxopt import matrix\n",
        "from cvxopt.solvers import qp as Solver, options as SolverOptions\n",
        "from scipy.optimize import minimize_scalar\n",
        "from scipy.optimize import minimize\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import pandas as pd\n",
        "\n",
        "# Pull the data (from sbb.py and gold.py from website codebase)\n",
        "# Stocks, bonds, bills\n",
        "nominal = pd.read_csv('https://www.dropbox.com/s/hgwte6swx57jqcv/nominal_sbb.csv?dl=1', index_col=['Year'])\n",
        "\n",
        "# Gold\n",
        "d = quandl.get(\"LBMA/GOLD\")['USD (AM)']\n",
        "gold = d.resample('Y').last().iloc[:-1]\n",
        "gold.index = [x.year for x in gold.index]\n",
        "gold.loc[1967] = d.iloc[0]\n",
        "gold = gold.sort_index().pct_change().dropna()\n",
        "gold.name = 'Gold'\n",
        " \n",
        "\n",
        "df = pd.concat((nominal, gold), axis=1).dropna()\n",
        "assets = ['TBills','S&P 500', 'Gold', 'Corporates', 'Treasuries']\n",
        "df = df[assets]\n",
        "\n",
        "##### Inputs\n",
        "# Window length (and initial period)\n",
        "window = 20\n",
        "n = len(assets)-1\n",
        "raver = 5\n",
        "short_lb = None\n",
        "T = len(df)-window\n",
        "\n",
        "\n",
        "\n",
        "# Rolling input estimation\n",
        "risky = assets[1:]\n",
        "df.columns = ['rf']+['r'+str(i) for i in range(n)]\n",
        "asset_list = [str(i) for i in range(n)]\n",
        "for asset in asset_list:\n",
        "    df['mn' + asset]=df['r'+asset].rolling(window).mean()\n",
        "    df['sd' + asset]=df['r'+asset].rolling(window).std()\n",
        "\n",
        "ret_list = ['r' + asset for asset in asset_list]\n",
        "corrs = df[ret_list].rolling(window, min_periods=window).corr()\n",
        "\n",
        "corr_list = []\n",
        "for j, asset in enumerate(asset_list):\n",
        "    for k in range(j+1,n):\n",
        "        df['c'+asset+str(k)]=corrs.loc[(slice(None),'r'+asset),'r'+str(k)].values\n",
        "df['year'] = df.index\n",
        "df = df.reset_index()\n",
        "\n",
        "\n",
        "# Prepare columns for the rolling optimization output\n",
        "model_list = ['ew', 'est_all', 'est_cov', 'est_sds']\n",
        "for model in model_list:\n",
        "    df['portret_'+model] = np.nan      #portret is the realized portfolio return of the 100% risky asset portfolio\n",
        "    if model not in ['ew']:\n",
        "        for asset in asset_list:\n",
        "            df['wgt' + asset + '_' +model] = np.nan\n",
        "    df['wgt_cal_'+model] =np.nan\n",
        "    df['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor\n",
        "\n",
        "mn_list = ['mn'+asset for asset in asset_list]\n",
        "sd_list = ['sd'+asset for asset in asset_list] \n",
        "\n",
        "# Choose optimal portfolios each time period\n",
        "for i in np.arange(window,window+T):\n",
        "    # Full estimation inputs at each point in time\n",
        "    means = df[mn_list].iloc[i-1].values\n",
        "    sds   = df[sd_list].iloc[i-1].values\n",
        "    C  = np.identity(n)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        for k in range(j+1,n):\n",
        "            C[j, k] = C[k, j] =    df.loc[i-1,'c'+asset+str(k)]  \n",
        "    cov = np.diag(sds) @ C @ np.diag(sds)\n",
        "\n",
        "    r = df.loc[i,'rf']\n",
        "    ##### Note: all portfolio weights considered to be beginning of period weights\n",
        "    ##### (so multiply by contemporaneous realized returns)\n",
        "    # Full estimation tangency portfolio\n",
        "    model = 'est_all'\n",
        "    wgts = tangency(means,cov,r,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    df.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))\n",
        "    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "    # Estimate only covariance matrix\n",
        "    model = 'est_cov'\n",
        "    wgts = gmv(cov,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)\n",
        "\n",
        "\n",
        "    # Estimate only standard deviations in covariance matrix\n",
        "    model = 'est_sds'\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        for k in range(j+1,n):\n",
        "            cov[j, k] = cov[k, j] = 0.0\n",
        "    wgts = gmv(cov,short_lb)\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\t\t\n",
        "    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    \n",
        "\n",
        "    # Equal-weighted portfolio\n",
        "    model = 'ew'\n",
        "    for j, asset in enumerate(asset_list):\n",
        "        cov[j,j] = (sds.mean())**2\n",
        "    wgts = (1/n)*np.ones(n)\n",
        "    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts\n",
        "    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))\n",
        "    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])\n",
        "    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   \n",
        "\n",
        "# Summarize sharpe ratio, avg ret, sd(ret) for each model\n",
        "portret_list = ['raver_portret_' +  model for model in ['est_all', 'est_cov', 'est_sds','ew']]\n",
        "stats = df[portret_list].describe()\n",
        "sr_df = pd.DataFrame(dtype=float, columns = ['sr','avg_ret','sd_ret'], index = ['est_all', 'est_cov', 'est_sds','ew'])\n",
        "r = df[np.isnan(df['raver_portret_ew'])==False].rf.mean()\n",
        "for model in ['est_all', 'est_cov', 'est_sds','ew']:\n",
        "    sr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]\n",
        "    sr_df.loc[model,'avg_ret'] = stats.loc['mean','raver_portret_' +  model]\n",
        "    sr_df.loc[model,'sd_ret'] = stats.loc['std','raver_portret_' +  model]\n",
        "\n",
        "label_dict = {'true': 'theoretical optimal weights', \n",
        "            'ew': 'equal weights',\n",
        "            'est_all': 'estimate all inputs',\n",
        "            'est_cov': 'estimate covariance matrix only',\n",
        "            'est_sds': 'estimate standard deviations only'}\n",
        "\n",
        "xaxis_label_dict = {'true': 'theoretical optimal weights', \n",
        "            'ew': 'Equal Weights',\n",
        "            'est_all': 'Tangency',\n",
        "            'est_cov': 'GMV',\n",
        "            'est_sds': 'Risk Parity'}\n",
        "sr_df = sr_df.reset_index()\n",
        "sr_df['label'] = sr_df['index'].apply(lambda x: label_dict[x])\n",
        "sr_df['xaxis_label'] = sr_df['index'].apply(lambda x: xaxis_label_dict[x])\n",
        "\n",
        "\n",
        "# Plot sharpe ratios\n",
        "string =\"Strategy: %{customdata[0]} <br>\"\n",
        "string += \"Sharpe ratio: %{y:0.3f}<br>\"\n",
        "string += \"Average return: %{customdata[1]:0.1%}<br>\"\n",
        "string += \"SD(return): %{customdata[2]:0.1%}<br>\"\n",
        "string += \"<extra></extra>\"\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=sr_df['xaxis_label'], y=sr_df['sr'], customdata=sr_df[['label','avg_ret','sd_ret']], hovertemplate=string))\n",
        "fig.layout.yaxis[\"title\"] = \"Sharpe ratio\"\n",
        "fig.layout.xaxis[\"title\"] = \"Strategy\"\n",
        "fig.show()"
      ],
      "id": "fig-stockbondsgold-sharpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@fig-stockbondsgold-timeseries plots the realized returns for each strategy.  The hoverdata contains the optimal risky portfolio weights as well as the fraction of capital allocated to the risky portfolio.  The tangency and GMV strategies produce the most volatile return series.  However, as seen in @fig-stockbondsgold-sharpe, they do not produce average returns high enough to make up for this additional risk to generate Sharpe ratios as high as the risk parity or equal-weighted $1/N$ strategies.  Attempting to estimate expected returns and correlations for these assets would not have paid off.\n"
      ],
      "id": "4e9ebbf5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-stockbondsgold-timeseries\n",
        "#| fig-cap: Returns and weights of various strategies historically\n",
        "# Plot the time-series of returns and portfolio weights.\n",
        "for asset in asset_list:\n",
        "    df['wgt'+asset + '_ew'] = 1/n\n",
        "       \n",
        "fig = go.Figure()\n",
        "for model in ['est_all', 'est_cov', 'est_sds', 'ew']:\n",
        "    string =  \"Strategy: \" + label_dict[model] +\" <br>\"\n",
        "    string += \"Year: %{x:4.0f}<br>\"\n",
        "    string += \"Return: %{y:0.1%}<br>\"\n",
        "    string += \"Weight in Risky Portfolio: %{customdata[0]: 0.1%} <br>\"\n",
        "    string += \"Risky Portfolio Weights:<br>\"\n",
        "    string += \"  \"+ risky[0] +\": %{customdata[1]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[1] +\": %{customdata[2]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[2] +\": %{customdata[3]: 0.1%} <br>\"\n",
        "    string += \"  \"+ risky[3] +\": %{customdata[4]: 0.1%} <br>\"\n",
        "    string += \"<extra></extra>\"\n",
        "\n",
        "    wgt_list = ['wgt_cal_'+ model] + ['wgt'+asset + \"_\" + model for asset in asset_list]\n",
        "    trace=go.Scatter(x=df['year'], y=df['raver_portret_'+model], customdata=df[wgt_list], hovertemplate=string, name = xaxis_label_dict[model])\n",
        "    fig.add_trace(trace)\n",
        "fig.layout.yaxis[\"title\"] = \"Return\"\n",
        "fig.layout.xaxis[\"title\"] = \"Year\"\n",
        "fig.update_yaxes(tickformat=\".0%\")\n",
        "fig.update_layout(legend=dict(yanchor=\"top\", y =0.99, xanchor=\"left\", x=0.01))\n",
        "fig.update_xaxes(range=[df['year'].iloc[window], df.year.max()])\n",
        "fig.show()"
      ],
      "id": "fig-stockbondsgold-timeseries",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}